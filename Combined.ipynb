{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ab2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import ast\n",
    "from nrclex import NRCLex\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58faf40e",
   "metadata": {},
   "source": [
    "### Feature Engineering Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01101f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_data_new_negation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d27a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized_processed_text\"] = df[\"lemmatized_processed_text\"].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e420b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_lemmatized_processed_text'] = [' '.join(map(str, l)) for l in df['lemmatized_processed_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8573475",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emotions'] = df['clean_lemmatized_processed_text'].apply(lambda x: NRCLex(x).affect_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2254268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.drop(['emotions'], axis = 1), df['emotions'].apply(pd.Series)], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd475a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c9eb6",
   "metadata": {},
   "source": [
    "### Feature Engineering Dictionary + POS_Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00928adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVAHOME'] = 'C:/Program Files/Java/jdk-17.0.1/bin'\n",
    "os.environ['STANFORD_PARSER'] = 'C:/stanford-corenlp-4.5.3'\n",
    "os.environ['STANFORD_MODELS'] = 'C:/stanford-corenlp-4.5.3'\n",
    "\n",
    "# os.environ['JAVAHOME'] = 'C:/Program Files/Java/jdk1.8.0_361/bin'\n",
    "# os.environ['STANFORD_PARSER'] = 'C:/Program Files/stanford-corenlp-4.5.2/'\n",
    "# os.environ['STANFORD_MODELS'] = 'C:/Program Files/stanford-corenlp-4.5.2/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59f3942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser\n",
    "\n",
    "pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dbe238",
   "metadata": {},
   "source": [
    "### Domain Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75d8c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = pd.read_csv(\"suicidal_indicator.csv\", header=None).T\n",
    "dict2 = pd.read_csv(\"suicidal_ideation.csv\", header=None).T\n",
    "dict3 = pd.read_csv(\"suicidal_behavior.csv\", header=None).T\n",
    "dict4 = pd.read_csv(\"suicidal_attempt.csv\", header=None).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289291e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pessimistic character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suicide of relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Family history of suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suicide of close relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suicide risk assessment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     lexicons\n",
       "0       Pessimistic character\n",
       "1         Suicide of relative\n",
       "2   Family history of suicide\n",
       "3   Suicide of close relative\n",
       "4     Suicide risk assessment"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###combined dictionary into 1\n",
    "domain_dict = pd.concat([dict1, dict2, dict3,dict4], ignore_index=True)\n",
    "domain_dict = domain_dict.rename(columns={0: 'lexicons'})\n",
    "domain_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "400cd5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_text(text):\n",
    "    text = str(text).lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89450e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict['lexicons'] = domain_dict['lexicons'].apply(lower_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3da51321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pessimistic character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suicide of relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family history of suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide of close relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide risk assessment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>went in the freezer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>jumped from bridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>jumped from roof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>bag around head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>belt around neck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2277 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        lexicons\n",
       "0          pessimistic character\n",
       "1            suicide of relative\n",
       "2      family history of suicide\n",
       "3      suicide of close relative\n",
       "4        suicide risk assessment\n",
       "...                          ...\n",
       "2272         went in the freezer\n",
       "2273          jumped from bridge\n",
       "2274            jumped from roof\n",
       "2275             bag around head\n",
       "2276            belt around neck\n",
       "\n",
       "[2277 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96aed1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict = domain_dict.drop(domain_dict.index[1521]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b30c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_preprocess_text(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    sentence = str(sentence)\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    tagged_words = pos_tagger.tag(words)\n",
    "    tagged_words = [(stemmer.stem(word), tag) for word, tag in tagged_words if word not in stop_words]\n",
    "    return tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf6cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict[\"stem\"] = domain_dict[\"lexicons\"].apply(stem_preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121b889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_preprocess_text(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "#     stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = str(sentence)\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    tagged_words = pos_tagger.tag(words)\n",
    "    tagged_words = [(lemmatizer.lemmatize(word), tag) for word, tag in tagged_words if word not in stop_words]\n",
    "    return tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f5464dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict[\"lem\"] = domain_dict[\"lexicons\"].apply(lem_preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fee2012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "      <th>stem</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pessimistic character</td>\n",
       "      <td>[(pessimist, JJ), (charact, NN)]</td>\n",
       "      <td>[(pessimistic, JJ), (character, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suicide of relative</td>\n",
       "      <td>[(suicid, NN), (relat, JJ)]</td>\n",
       "      <td>[(suicide, NN), (relative, JJ)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family history of suicide</td>\n",
       "      <td>[(famili, NN), (histori, NN), (suicid, NN)]</td>\n",
       "      <td>[(family, NN), (history, NN), (suicide, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide of close relative</td>\n",
       "      <td>[(suicid, NN), (close, JJ), (relat, JJ)]</td>\n",
       "      <td>[(suicide, NN), (close, JJ), (relative, JJ)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide risk assessment</td>\n",
       "      <td>[(suicid, NN), (risk, NN), (assess, NN)]</td>\n",
       "      <td>[(suicide, NN), (risk, NN), (assessment, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>went in the freezer</td>\n",
       "      <td>[(went, VBD), (freezer, NN)]</td>\n",
       "      <td>[(went, VBD), (freezer, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>jumped from bridge</td>\n",
       "      <td>[(jump, VBD), (bridg, NN)]</td>\n",
       "      <td>[(jumped, VBD), (bridge, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>jumped from roof</td>\n",
       "      <td>[(jump, VBD), (roof, NN)]</td>\n",
       "      <td>[(jumped, VBD), (roof, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>bag around head</td>\n",
       "      <td>[(bag, NN), (around, IN), (head, NN)]</td>\n",
       "      <td>[(bag, NN), (around, IN), (head, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>belt around neck</td>\n",
       "      <td>[(belt, NN), (around, IN), (neck, NN)]</td>\n",
       "      <td>[(belt, NN), (around, IN), (neck, NN)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2276 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        lexicons                                         stem  \\\n",
       "0          pessimistic character             [(pessimist, JJ), (charact, NN)]   \n",
       "1            suicide of relative                  [(suicid, NN), (relat, JJ)]   \n",
       "2      family history of suicide  [(famili, NN), (histori, NN), (suicid, NN)]   \n",
       "3      suicide of close relative     [(suicid, NN), (close, JJ), (relat, JJ)]   \n",
       "4        suicide risk assessment     [(suicid, NN), (risk, NN), (assess, NN)]   \n",
       "...                          ...                                          ...   \n",
       "2271         went in the freezer                 [(went, VBD), (freezer, NN)]   \n",
       "2272          jumped from bridge                   [(jump, VBD), (bridg, NN)]   \n",
       "2273            jumped from roof                    [(jump, VBD), (roof, NN)]   \n",
       "2274             bag around head        [(bag, NN), (around, IN), (head, NN)]   \n",
       "2275            belt around neck       [(belt, NN), (around, IN), (neck, NN)]   \n",
       "\n",
       "                                                lem  \n",
       "0              [(pessimistic, JJ), (character, NN)]  \n",
       "1                   [(suicide, NN), (relative, JJ)]  \n",
       "2      [(family, NN), (history, NN), (suicide, NN)]  \n",
       "3      [(suicide, NN), (close, JJ), (relative, JJ)]  \n",
       "4     [(suicide, NN), (risk, NN), (assessment, NN)]  \n",
       "...                                             ...  \n",
       "2271                   [(went, VBD), (freezer, NN)]  \n",
       "2272                  [(jumped, VBD), (bridge, NN)]  \n",
       "2273                    [(jumped, VBD), (roof, NN)]  \n",
       "2274          [(bag, NN), (around, IN), (head, NN)]  \n",
       "2275         [(belt, NN), (around, IN), (neck, NN)]  \n",
       "\n",
       "[2276 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc4f7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_list_to_string(nested_list):\n",
    "    return ' '.join(['_'.join(tup) for tup in nested_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "682e6cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "      <th>stem</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pessimistic character</td>\n",
       "      <td>pessimist_JJ charact_NN</td>\n",
       "      <td>pessimistic_JJ character_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suicide of relative</td>\n",
       "      <td>suicid_NN relat_JJ</td>\n",
       "      <td>suicide_NN relative_JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family history of suicide</td>\n",
       "      <td>famili_NN histori_NN suicid_NN</td>\n",
       "      <td>family_NN history_NN suicide_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide of close relative</td>\n",
       "      <td>suicid_NN close_JJ relat_JJ</td>\n",
       "      <td>suicide_NN close_JJ relative_JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide risk assessment</td>\n",
       "      <td>suicid_NN risk_NN assess_NN</td>\n",
       "      <td>suicide_NN risk_NN assessment_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>went in the freezer</td>\n",
       "      <td>went_VBD freezer_NN</td>\n",
       "      <td>went_VBD freezer_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>jumped from bridge</td>\n",
       "      <td>jump_VBD bridg_NN</td>\n",
       "      <td>jumped_VBD bridge_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>jumped from roof</td>\n",
       "      <td>jump_VBD roof_NN</td>\n",
       "      <td>jumped_VBD roof_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>bag around head</td>\n",
       "      <td>bag_NN around_IN head_NN</td>\n",
       "      <td>bag_NN around_IN head_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>belt around neck</td>\n",
       "      <td>belt_NN around_IN neck_NN</td>\n",
       "      <td>belt_NN around_IN neck_NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2276 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        lexicons                            stem  \\\n",
       "0          pessimistic character         pessimist_JJ charact_NN   \n",
       "1            suicide of relative              suicid_NN relat_JJ   \n",
       "2      family history of suicide  famili_NN histori_NN suicid_NN   \n",
       "3      suicide of close relative     suicid_NN close_JJ relat_JJ   \n",
       "4        suicide risk assessment     suicid_NN risk_NN assess_NN   \n",
       "...                          ...                             ...   \n",
       "2271         went in the freezer             went_VBD freezer_NN   \n",
       "2272          jumped from bridge               jump_VBD bridg_NN   \n",
       "2273            jumped from roof                jump_VBD roof_NN   \n",
       "2274             bag around head        bag_NN around_IN head_NN   \n",
       "2275            belt around neck       belt_NN around_IN neck_NN   \n",
       "\n",
       "                                   lem  \n",
       "0          pessimistic_JJ character_NN  \n",
       "1               suicide_NN relative_JJ  \n",
       "2      family_NN history_NN suicide_NN  \n",
       "3      suicide_NN close_JJ relative_JJ  \n",
       "4     suicide_NN risk_NN assessment_NN  \n",
       "...                                ...  \n",
       "2271               went_VBD freezer_NN  \n",
       "2272              jumped_VBD bridge_NN  \n",
       "2273                jumped_VBD roof_NN  \n",
       "2274          bag_NN around_IN head_NN  \n",
       "2275         belt_NN around_IN neck_NN  \n",
       "\n",
       "[2276 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dict[\"stem\"] = domain_dict[\"stem\"].apply(nested_list_to_string)\n",
    "domain_dict[\"lem\"] = domain_dict[\"lem\"].apply(nested_list_to_string)\n",
    "domain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dec94099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2276"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get list of domain dictionary that are stemmed\n",
    "list_of_stem = []\n",
    "for i in range (len(domain_dict[\"stem\"])):\n",
    "    list_of_stem.append(domain_dict[\"stem\"].iloc[i])\n",
    "len(list_of_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38ee2fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2276"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get list of domain dictionary that are lemmatized\n",
    "list_of_lem = []\n",
    "for j in range (len(domain_dict[\"lem\"])):\n",
    "    list_of_lem.append(domain_dict[\"lem\"].iloc[j])\n",
    "len(list_of_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f8e710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"domain_dict_postag_lem.txt\", \"w\") as file:\n",
    "    for item in list_of_lem:\n",
    "        file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d97387",
   "metadata": {},
   "source": [
    "### POS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6bb402c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>stemmed_processed_text</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently i left my...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threaten', 'suicid', 'recent',...</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suicide', 'rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'not', 'get', 'affect', 'compliment'...</td>\n",
       "      <td>['weird', 'not', 'get', 'affected', 'complimen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally is almost over so i can never hear has...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['final', 'almost', 'never', 'hear', 'bad', 'y...</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need help just help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'help', 'help', 'cri', 'hard']</td>\n",
       "      <td>['need', 'help', 'help', 'cry', 'hard']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m so lost hello my name is adam and i ve bee...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggl', '...</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggling'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class  \\\n",
       "0  ex wife threatening suicide recently i left my...      suicide   \n",
       "1  am i weird i do not get affected by compliment...  non-suicide   \n",
       "2  finally is almost over so i can never hear has...  non-suicide   \n",
       "3       i need help just help me i am crying so hard      suicide   \n",
       "4  i m so lost hello my name is adam and i ve bee...      suicide   \n",
       "\n",
       "                              stemmed_processed_text  \\\n",
       "0  ['ex', 'wife', 'threaten', 'suicid', 'recent',...   \n",
       "1  ['weird', 'not', 'get', 'affect', 'compliment'...   \n",
       "2  ['final', 'almost', 'never', 'hear', 'bad', 'y...   \n",
       "3            ['need', 'help', 'help', 'cri', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggl', '...   \n",
       "\n",
       "                           lemmatized_processed_text  \n",
       "0  ['ex', 'wife', 'threatening', 'suicide', 'rece...  \n",
       "1  ['weird', 'not', 'get', 'affected', 'complimen...  \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...  \n",
       "3            ['need', 'help', 'help', 'cry', 'hard']  \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggling'...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"preprocessed_data_new_negation.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef373a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 232074 entries, 0 to 232073\n",
      "Data columns (total 4 columns):\n",
      " #   Column                     Non-Null Count   Dtype \n",
      "---  ------                     --------------   ----- \n",
      " 0   text                       232017 non-null  object\n",
      " 1   class                      232074 non-null  object\n",
      " 2   stemmed_processed_text     232074 non-null  object\n",
      " 3   lemmatized_processed_text  232074 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 7.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6b2fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop stemmed_processed_text column\n",
    "data.drop(columns = [\"stemmed_processed_text\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cf3134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean and tokenize lemmatized_processed_text as it is intepreted as an entire string \n",
    "def clean_and_tokenize(text):\n",
    "    # Remove the brackets and commas using a regular expression\n",
    "    cleaned_text = re.sub(r\"[\\[\\],']\", \"\", text)\n",
    "    # Tokenize the string using word_tokenize\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f51c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'lemmatized_processed_text' column of the DataFrame\n",
    "data['tokens'] = data['lemmatized_processed_text'].apply(clean_and_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fb87977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add POS tags to a list of tokens\n",
    "def add_pos_tags(tokens):\n",
    "    tagged_tokens = []\n",
    "    for token in tokens:\n",
    "        pos_tag = pos_tagger.tag([token])[0][1]\n",
    "        tagged_token = f\"{token}_{pos_tag}\"\n",
    "        tagged_tokens.append(tagged_token)\n",
    "    return tagged_tokens\n",
    "\n",
    "# Apply the function to the 'tokens' column of the DataFrame\n",
    "data['tagged_tokens'] = data['tokens'].map(add_pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3041afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently i left my...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suicide', 'rece...</td>\n",
       "      <td>[ex, wife, threatening, suicide, recently, lef...</td>\n",
       "      <td>[ex_NN, wife_NN, threatening_VBG, suicide_NN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'not', 'get', 'affected', 'complimen...</td>\n",
       "      <td>[weird, not, get, affected, compliment, coming...</td>\n",
       "      <td>[weird_JJ, not_RB, get_VB, affected_VBN, compl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally is almost over so i can never hear has...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>[finally, almost, never, hear, bad, year, ever...</td>\n",
       "      <td>[finally_RB, almost_RB, never_RB, hear_VB, bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need help just help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'help', 'help', 'cry', 'hard']</td>\n",
       "      <td>[need, help, help, cry, hard]</td>\n",
       "      <td>[need_NN, help_NN, help_NN, cry_NN, hard_RB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m so lost hello my name is adam and i ve bee...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggling'...</td>\n",
       "      <td>[lost, hello, name, adam, struggling, year, af...</td>\n",
       "      <td>[lost_VBN, hello_UH, name_NN, adam_NN, struggl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class  \\\n",
       "0  ex wife threatening suicide recently i left my...      suicide   \n",
       "1  am i weird i do not get affected by compliment...  non-suicide   \n",
       "2  finally is almost over so i can never hear has...  non-suicide   \n",
       "3       i need help just help me i am crying so hard      suicide   \n",
       "4  i m so lost hello my name is adam and i ve bee...      suicide   \n",
       "\n",
       "                           lemmatized_processed_text  \\\n",
       "0  ['ex', 'wife', 'threatening', 'suicide', 'rece...   \n",
       "1  ['weird', 'not', 'get', 'affected', 'complimen...   \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3            ['need', 'help', 'help', 'cry', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggling'...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [ex, wife, threatening, suicide, recently, lef...   \n",
       "1  [weird, not, get, affected, compliment, coming...   \n",
       "2  [finally, almost, never, hear, bad, year, ever...   \n",
       "3                      [need, help, help, cry, hard]   \n",
       "4  [lost, hello, name, adam, struggling, year, af...   \n",
       "\n",
       "                                       tagged_tokens  \n",
       "0  [ex_NN, wife_NN, threatening_VBG, suicide_NN, ...  \n",
       "1  [weird_JJ, not_RB, get_VB, affected_VBN, compl...  \n",
       "2  [finally_RB, almost_RB, never_RB, hear_VB, bad...  \n",
       "3       [need_NN, help_NN, help_NN, cry_NN, hard_RB]  \n",
       "4  [lost_VBN, hello_UH, name_NN, adam_NN, struggl...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbc2b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('pos_tagged_new_negation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2858d",
   "metadata": {},
   "source": [
    "### Combining All FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d4fa64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.read_csv('pos_tagged_new_negation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d291a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bd5b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df['tagged_tokens'] = pos_df['tagged_tokens'].apply(lambda x: [str(i) for i in ast.literal_eval(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76502efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>suicide</td>\n",
       "      <td>ex_NN wife_NN threatening_VBG suicide_NN recen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>weird_JJ not_RB get_VB affected_VBN compliment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>finally_RB almost_RB never_RB hear_VB bad_JJ y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide</td>\n",
       "      <td>need_NN help_NN help_NN cry_NN hard_RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide</td>\n",
       "      <td>lost_VBN hello_UH name_NN adam_NN struggling_V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class                                      tagged_tokens\n",
       "0      suicide  ex_NN wife_NN threatening_VBG suicide_NN recen...\n",
       "1  non-suicide  weird_JJ not_RB get_VB affected_VBN compliment...\n",
       "2  non-suicide  finally_RB almost_RB never_RB hear_VB bad_JJ y...\n",
       "3      suicide             need_NN help_NN help_NN cry_NN hard_RB\n",
       "4      suicide  lost_VBN hello_UH name_NN adam_NN struggling_V..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df.drop(columns = [\"text\",\"lemmatized_processed_text\",\"tokens\"], inplace = True)\n",
    "pos_df[\"tagged_tokens\"] = pos_df[\"tagged_tokens\"].apply(lambda x: \" \".join(x))\n",
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be754a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>stemmed_processed_text</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ex wife threatening suicide recently i left my...</td>\n",
       "      <td>1</td>\n",
       "      <td>['ex', 'wife', 'threaten', 'suicid', 'recent',...</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suicide', 'rece...</td>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>0</td>\n",
       "      <td>['weird', 'not', 'get', 'affect', 'compliment'...</td>\n",
       "      <td>['weird', 'not', 'get', 'affected', 'complimen...</td>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>finally is almost over so i can never hear has...</td>\n",
       "      <td>0</td>\n",
       "      <td>['final', 'almost', 'never', 'hear', 'bad', 'y...</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i need help just help me i am crying so hard</td>\n",
       "      <td>1</td>\n",
       "      <td>['need', 'help', 'help', 'cri', 'hard']</td>\n",
       "      <td>['need', 'help', 'help', 'cry', 'hard']</td>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i m so lost hello my name is adam and i ve bee...</td>\n",
       "      <td>1</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggl', '...</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggling'...</td>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  class  \\\n",
       "0           0  ex wife threatening suicide recently i left my...      1   \n",
       "1           1  am i weird i do not get affected by compliment...      0   \n",
       "2           2  finally is almost over so i can never hear has...      0   \n",
       "3           3       i need help just help me i am crying so hard      1   \n",
       "4           4  i m so lost hello my name is adam and i ve bee...      1   \n",
       "\n",
       "                              stemmed_processed_text  \\\n",
       "0  ['ex', 'wife', 'threaten', 'suicid', 'recent',...   \n",
       "1  ['weird', 'not', 'get', 'affect', 'compliment'...   \n",
       "2  ['final', 'almost', 'never', 'hear', 'bad', 'y...   \n",
       "3            ['need', 'help', 'help', 'cri', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggl', '...   \n",
       "\n",
       "                           lemmatized_processed_text  \\\n",
       "0  ['ex', 'wife', 'threatening', 'suicide', 'rece...   \n",
       "1  ['weird', 'not', 'get', 'affected', 'complimen...   \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3            ['need', 'help', 'help', 'cry', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggling'...   \n",
       "\n",
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation  \n",
       "0  0.078125      0.109375  \n",
       "1  0.133333      0.200000  \n",
       "2  0.100000      0.100000  \n",
       "3  0.000000      0.000000  \n",
       "4  0.042453      0.122642  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df = pd.read_csv(\"df_emotions.csv\")\n",
    "emotion_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5473d640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation  \n",
       "0  0.078125      0.109375  \n",
       "1  0.133333      0.200000  \n",
       "2  0.100000      0.100000  \n",
       "3  0.000000      0.000000  \n",
       "4  0.042453      0.122642  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df.drop(columns = [\"text\",\"lemmatized_processed_text\",\"class\",\"stemmed_processed_text\",\"Unnamed: 0\"],axis=1,inplace=True)\n",
    "emotion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe6a86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([emotion_df, pos_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b9e0798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>class</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>suicide</td>\n",
       "      <td>ex_NN wife_NN threatening_VBG suicide_NN recen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>weird_JJ not_RB get_VB affected_VBN compliment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>finally_RB almost_RB never_RB hear_VB bad_JJ y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>suicide</td>\n",
       "      <td>need_NN help_NN help_NN cry_NN hard_RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>suicide</td>\n",
       "      <td>lost_VBN hello_UH name_NN adam_NN struggling_V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation        class  \\\n",
       "0  0.078125      0.109375      suicide   \n",
       "1  0.133333      0.200000  non-suicide   \n",
       "2  0.100000      0.100000  non-suicide   \n",
       "3  0.000000      0.000000      suicide   \n",
       "4  0.042453      0.122642      suicide   \n",
       "\n",
       "                                       tagged_tokens  \n",
       "0  ex_NN wife_NN threatening_VBG suicide_NN recen...  \n",
       "1  weird_JJ not_RB get_VB affected_VBN compliment...  \n",
       "2  finally_RB almost_RB never_RB hear_VB bad_JJ y...  \n",
       "3             need_NN help_NN help_NN cry_NN hard_RB  \n",
       "4  lost_VBN hello_UH name_NN adam_NN struggling_V...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5852ad",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed3429",
   "metadata": {},
   "source": [
    "### POS_TAG + Domain_Dictionary Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd91f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffa3a08d",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b06846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e80f4eb3",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81196ef",
   "metadata": {},
   "source": [
    "### POS_TAG + Domain_Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b63545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c143eb66",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2b035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2caf1d87",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7885a0",
   "metadata": {},
   "source": [
    "### POS_TAG + Domain_Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30a0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08963129",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e6b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "102c3085",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27953b4",
   "metadata": {},
   "source": [
    "### POS_TAG + Domain_Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba53be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2acafbdb",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e91df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
