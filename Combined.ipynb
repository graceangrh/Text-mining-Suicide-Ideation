{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ab2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import ast\n",
    "from nrclex import NRCLex\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58faf40e",
   "metadata": {},
   "source": [
    "### Feature Engineering Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01101f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_data_new_negation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d27a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized_processed_text\"] = df[\"lemmatized_processed_text\"].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e420b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_lemmatized_processed_text'] = [' '.join(map(str, l)) for l in df['lemmatized_processed_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8573475",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emotions'] = df['clean_lemmatized_processed_text'].apply(lambda x: NRCLex(x).affect_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2254268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.drop(['emotions'], axis = 1), df['emotions'].apply(pd.Series)], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd475a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c9eb6",
   "metadata": {},
   "source": [
    "### Feature Engineering Dictionary + POS_Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00928adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVAHOME'] = 'C:/Program Files/Java/jdk-17.0.1/bin'\n",
    "os.environ['STANFORD_PARSER'] = 'C:/stanford-corenlp-4.5.3'\n",
    "os.environ['STANFORD_MODELS'] = 'C:/stanford-corenlp-4.5.3'\n",
    "\n",
    "# os.environ['JAVAHOME'] = 'C:/Program Files/Java/jdk1.8.0_361/bin'\n",
    "# os.environ['STANFORD_PARSER'] = 'C:/Program Files/stanford-corenlp-4.5.2/'\n",
    "# os.environ['STANFORD_MODELS'] = 'C:/Program Files/stanford-corenlp-4.5.2/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59f3942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser\n",
    "\n",
    "pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dbe238",
   "metadata": {},
   "source": [
    "### Domain Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75d8c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = pd.read_csv(\"suicidal_indicator.csv\", header=None).T\n",
    "dict2 = pd.read_csv(\"suicidal_ideation.csv\", header=None).T\n",
    "dict3 = pd.read_csv(\"suicidal_behavior.csv\", header=None).T\n",
    "dict4 = pd.read_csv(\"suicidal_attempt.csv\", header=None).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289291e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pessimistic character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suicide of relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Family history of suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suicide of close relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suicide risk assessment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     lexicons\n",
       "0       Pessimistic character\n",
       "1         Suicide of relative\n",
       "2   Family history of suicide\n",
       "3   Suicide of close relative\n",
       "4     Suicide risk assessment"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###combined dictionary into 1\n",
    "domain_dict = pd.concat([dict1, dict2, dict3,dict4], ignore_index=True)\n",
    "domain_dict = domain_dict.rename(columns={0: 'lexicons'})\n",
    "domain_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "400cd5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_text(text):\n",
    "    text = str(text).lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89450e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict['lexicons'] = domain_dict['lexicons'].apply(lower_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3da51321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pessimistic character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suicide of relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family history of suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide of close relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide risk assessment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>went in the freezer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>jumped from bridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>jumped from roof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>bag around head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>belt around neck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2277 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        lexicons\n",
       "0          pessimistic character\n",
       "1            suicide of relative\n",
       "2      family history of suicide\n",
       "3      suicide of close relative\n",
       "4        suicide risk assessment\n",
       "...                          ...\n",
       "2272         went in the freezer\n",
       "2273          jumped from bridge\n",
       "2274            jumped from roof\n",
       "2275             bag around head\n",
       "2276            belt around neck\n",
       "\n",
       "[2277 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96aed1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict = domain_dict.drop(domain_dict.index[1521]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b30c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_preprocess_text(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    sentence = str(sentence)\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    tagged_words = pos_tagger.tag(words)\n",
    "    tagged_words = [(stemmer.stem(word), tag) for word, tag in tagged_words if word not in stop_words]\n",
    "    return tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf6cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict[\"stem\"] = domain_dict[\"lexicons\"].apply(stem_preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121b889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_preprocess_text(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "#     stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = str(sentence)\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    tagged_words = pos_tagger.tag(words)\n",
    "    tagged_words = [(lemmatizer.lemmatize(word), tag) for word, tag in tagged_words if word not in stop_words]\n",
    "    return tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f5464dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict[\"lem\"] = domain_dict[\"lexicons\"].apply(lem_preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fee2012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "      <th>stem</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pessimistic character</td>\n",
       "      <td>[(pessimist, JJ), (charact, NN)]</td>\n",
       "      <td>[(pessimistic, JJ), (character, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suicide of relative</td>\n",
       "      <td>[(suicid, NN), (relat, JJ)]</td>\n",
       "      <td>[(suicide, NN), (relative, JJ)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family history of suicide</td>\n",
       "      <td>[(famili, NN), (histori, NN), (suicid, NN)]</td>\n",
       "      <td>[(family, NN), (history, NN), (suicide, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide of close relative</td>\n",
       "      <td>[(suicid, NN), (close, JJ), (relat, JJ)]</td>\n",
       "      <td>[(suicide, NN), (close, JJ), (relative, JJ)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide risk assessment</td>\n",
       "      <td>[(suicid, NN), (risk, NN), (assess, NN)]</td>\n",
       "      <td>[(suicide, NN), (risk, NN), (assessment, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>went in the freezer</td>\n",
       "      <td>[(went, VBD), (freezer, NN)]</td>\n",
       "      <td>[(went, VBD), (freezer, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>jumped from bridge</td>\n",
       "      <td>[(jump, VBD), (bridg, NN)]</td>\n",
       "      <td>[(jumped, VBD), (bridge, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>jumped from roof</td>\n",
       "      <td>[(jump, VBD), (roof, NN)]</td>\n",
       "      <td>[(jumped, VBD), (roof, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>bag around head</td>\n",
       "      <td>[(bag, NN), (around, IN), (head, NN)]</td>\n",
       "      <td>[(bag, NN), (around, IN), (head, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>belt around neck</td>\n",
       "      <td>[(belt, NN), (around, IN), (neck, NN)]</td>\n",
       "      <td>[(belt, NN), (around, IN), (neck, NN)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2276 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        lexicons                                         stem  \\\n",
       "0          pessimistic character             [(pessimist, JJ), (charact, NN)]   \n",
       "1            suicide of relative                  [(suicid, NN), (relat, JJ)]   \n",
       "2      family history of suicide  [(famili, NN), (histori, NN), (suicid, NN)]   \n",
       "3      suicide of close relative     [(suicid, NN), (close, JJ), (relat, JJ)]   \n",
       "4        suicide risk assessment     [(suicid, NN), (risk, NN), (assess, NN)]   \n",
       "...                          ...                                          ...   \n",
       "2271         went in the freezer                 [(went, VBD), (freezer, NN)]   \n",
       "2272          jumped from bridge                   [(jump, VBD), (bridg, NN)]   \n",
       "2273            jumped from roof                    [(jump, VBD), (roof, NN)]   \n",
       "2274             bag around head        [(bag, NN), (around, IN), (head, NN)]   \n",
       "2275            belt around neck       [(belt, NN), (around, IN), (neck, NN)]   \n",
       "\n",
       "                                                lem  \n",
       "0              [(pessimistic, JJ), (character, NN)]  \n",
       "1                   [(suicide, NN), (relative, JJ)]  \n",
       "2      [(family, NN), (history, NN), (suicide, NN)]  \n",
       "3      [(suicide, NN), (close, JJ), (relative, JJ)]  \n",
       "4     [(suicide, NN), (risk, NN), (assessment, NN)]  \n",
       "...                                             ...  \n",
       "2271                   [(went, VBD), (freezer, NN)]  \n",
       "2272                  [(jumped, VBD), (bridge, NN)]  \n",
       "2273                    [(jumped, VBD), (roof, NN)]  \n",
       "2274          [(bag, NN), (around, IN), (head, NN)]  \n",
       "2275         [(belt, NN), (around, IN), (neck, NN)]  \n",
       "\n",
       "[2276 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc4f7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_list_to_string(nested_list):\n",
    "    return ' '.join(['_'.join(tup) for tup in nested_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "682e6cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "      <th>stem</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pessimistic character</td>\n",
       "      <td>pessimist_JJ charact_NN</td>\n",
       "      <td>pessimistic_JJ character_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suicide of relative</td>\n",
       "      <td>suicid_NN relat_JJ</td>\n",
       "      <td>suicide_NN relative_JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family history of suicide</td>\n",
       "      <td>famili_NN histori_NN suicid_NN</td>\n",
       "      <td>family_NN history_NN suicide_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide of close relative</td>\n",
       "      <td>suicid_NN close_JJ relat_JJ</td>\n",
       "      <td>suicide_NN close_JJ relative_JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide risk assessment</td>\n",
       "      <td>suicid_NN risk_NN assess_NN</td>\n",
       "      <td>suicide_NN risk_NN assessment_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>went in the freezer</td>\n",
       "      <td>went_VBD freezer_NN</td>\n",
       "      <td>went_VBD freezer_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>jumped from bridge</td>\n",
       "      <td>jump_VBD bridg_NN</td>\n",
       "      <td>jumped_VBD bridge_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>jumped from roof</td>\n",
       "      <td>jump_VBD roof_NN</td>\n",
       "      <td>jumped_VBD roof_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>bag around head</td>\n",
       "      <td>bag_NN around_IN head_NN</td>\n",
       "      <td>bag_NN around_IN head_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>belt around neck</td>\n",
       "      <td>belt_NN around_IN neck_NN</td>\n",
       "      <td>belt_NN around_IN neck_NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2276 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        lexicons                            stem  \\\n",
       "0          pessimistic character         pessimist_JJ charact_NN   \n",
       "1            suicide of relative              suicid_NN relat_JJ   \n",
       "2      family history of suicide  famili_NN histori_NN suicid_NN   \n",
       "3      suicide of close relative     suicid_NN close_JJ relat_JJ   \n",
       "4        suicide risk assessment     suicid_NN risk_NN assess_NN   \n",
       "...                          ...                             ...   \n",
       "2271         went in the freezer             went_VBD freezer_NN   \n",
       "2272          jumped from bridge               jump_VBD bridg_NN   \n",
       "2273            jumped from roof                jump_VBD roof_NN   \n",
       "2274             bag around head        bag_NN around_IN head_NN   \n",
       "2275            belt around neck       belt_NN around_IN neck_NN   \n",
       "\n",
       "                                   lem  \n",
       "0          pessimistic_JJ character_NN  \n",
       "1               suicide_NN relative_JJ  \n",
       "2      family_NN history_NN suicide_NN  \n",
       "3      suicide_NN close_JJ relative_JJ  \n",
       "4     suicide_NN risk_NN assessment_NN  \n",
       "...                                ...  \n",
       "2271               went_VBD freezer_NN  \n",
       "2272              jumped_VBD bridge_NN  \n",
       "2273                jumped_VBD roof_NN  \n",
       "2274          bag_NN around_IN head_NN  \n",
       "2275         belt_NN around_IN neck_NN  \n",
       "\n",
       "[2276 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dict[\"stem\"] = domain_dict[\"stem\"].apply(nested_list_to_string)\n",
    "domain_dict[\"lem\"] = domain_dict[\"lem\"].apply(nested_list_to_string)\n",
    "domain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dec94099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2276"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get list of domain dictionary that are stemmed\n",
    "list_of_stem = []\n",
    "for i in range (len(domain_dict[\"stem\"])):\n",
    "    list_of_stem.append(domain_dict[\"stem\"].iloc[i])\n",
    "len(list_of_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38ee2fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2276"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get list of domain dictionary that are lemmatized\n",
    "list_of_lem = []\n",
    "for j in range (len(domain_dict[\"lem\"])):\n",
    "    list_of_lem.append(domain_dict[\"lem\"].iloc[j])\n",
    "len(list_of_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f8e710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"domain_dict_postag_lem.txt\", \"w\") as file:\n",
    "    for item in list_of_lem:\n",
    "        file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d97387",
   "metadata": {},
   "source": [
    "### POS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6bb402c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>stemmed_processed_text</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently i left my...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threaten', 'suicid', 'recent',...</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suicide', 'rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'not', 'get', 'affect', 'compliment'...</td>\n",
       "      <td>['weird', 'not', 'get', 'affected', 'complimen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally is almost over so i can never hear has...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['final', 'almost', 'never', 'hear', 'bad', 'y...</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need help just help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'help', 'help', 'cri', 'hard']</td>\n",
       "      <td>['need', 'help', 'help', 'cry', 'hard']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m so lost hello my name is adam and i ve bee...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggl', '...</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggling'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class  \\\n",
       "0  ex wife threatening suicide recently i left my...      suicide   \n",
       "1  am i weird i do not get affected by compliment...  non-suicide   \n",
       "2  finally is almost over so i can never hear has...  non-suicide   \n",
       "3       i need help just help me i am crying so hard      suicide   \n",
       "4  i m so lost hello my name is adam and i ve bee...      suicide   \n",
       "\n",
       "                              stemmed_processed_text  \\\n",
       "0  ['ex', 'wife', 'threaten', 'suicid', 'recent',...   \n",
       "1  ['weird', 'not', 'get', 'affect', 'compliment'...   \n",
       "2  ['final', 'almost', 'never', 'hear', 'bad', 'y...   \n",
       "3            ['need', 'help', 'help', 'cri', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggl', '...   \n",
       "\n",
       "                           lemmatized_processed_text  \n",
       "0  ['ex', 'wife', 'threatening', 'suicide', 'rece...  \n",
       "1  ['weird', 'not', 'get', 'affected', 'complimen...  \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...  \n",
       "3            ['need', 'help', 'help', 'cry', 'hard']  \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggling'...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"preprocessed_data_new_negation.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef373a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 232074 entries, 0 to 232073\n",
      "Data columns (total 4 columns):\n",
      " #   Column                     Non-Null Count   Dtype \n",
      "---  ------                     --------------   ----- \n",
      " 0   text                       232017 non-null  object\n",
      " 1   class                      232074 non-null  object\n",
      " 2   stemmed_processed_text     232074 non-null  object\n",
      " 3   lemmatized_processed_text  232074 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 7.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6b2fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop stemmed_processed_text column\n",
    "data.drop(columns = [\"stemmed_processed_text\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cf3134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean and tokenize lemmatized_processed_text as it is intepreted as an entire string \n",
    "def clean_and_tokenize(text):\n",
    "    # Remove the brackets and commas using a regular expression\n",
    "    cleaned_text = re.sub(r\"[\\[\\],']\", \"\", text)\n",
    "    # Tokenize the string using word_tokenize\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f51c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'lemmatized_processed_text' column of the DataFrame\n",
    "data['tokens'] = data['lemmatized_processed_text'].apply(clean_and_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fb87977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add POS tags to a list of tokens\n",
    "def add_pos_tags(tokens):\n",
    "    tagged_tokens = []\n",
    "    for token in tokens:\n",
    "        pos_tag = pos_tagger.tag([token])[0][1]\n",
    "        tagged_token = f\"{token}_{pos_tag}\"\n",
    "        tagged_tokens.append(tagged_token)\n",
    "    return tagged_tokens\n",
    "\n",
    "# Apply the function to the 'tokens' column of the DataFrame\n",
    "data['tagged_tokens'] = data['tokens'].map(add_pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3041afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently i left my...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suicide', 'rece...</td>\n",
       "      <td>[ex, wife, threatening, suicide, recently, lef...</td>\n",
       "      <td>[ex_NN, wife_NN, threatening_VBG, suicide_NN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'not', 'get', 'affected', 'complimen...</td>\n",
       "      <td>[weird, not, get, affected, compliment, coming...</td>\n",
       "      <td>[weird_JJ, not_RB, get_VB, affected_VBN, compl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally is almost over so i can never hear has...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>[finally, almost, never, hear, bad, year, ever...</td>\n",
       "      <td>[finally_RB, almost_RB, never_RB, hear_VB, bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need help just help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'help', 'help', 'cry', 'hard']</td>\n",
       "      <td>[need, help, help, cry, hard]</td>\n",
       "      <td>[need_NN, help_NN, help_NN, cry_NN, hard_RB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m so lost hello my name is adam and i ve bee...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggling'...</td>\n",
       "      <td>[lost, hello, name, adam, struggling, year, af...</td>\n",
       "      <td>[lost_VBN, hello_UH, name_NN, adam_NN, struggl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class  \\\n",
       "0  ex wife threatening suicide recently i left my...      suicide   \n",
       "1  am i weird i do not get affected by compliment...  non-suicide   \n",
       "2  finally is almost over so i can never hear has...  non-suicide   \n",
       "3       i need help just help me i am crying so hard      suicide   \n",
       "4  i m so lost hello my name is adam and i ve bee...      suicide   \n",
       "\n",
       "                           lemmatized_processed_text  \\\n",
       "0  ['ex', 'wife', 'threatening', 'suicide', 'rece...   \n",
       "1  ['weird', 'not', 'get', 'affected', 'complimen...   \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3            ['need', 'help', 'help', 'cry', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggling'...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [ex, wife, threatening, suicide, recently, lef...   \n",
       "1  [weird, not, get, affected, compliment, coming...   \n",
       "2  [finally, almost, never, hear, bad, year, ever...   \n",
       "3                      [need, help, help, cry, hard]   \n",
       "4  [lost, hello, name, adam, struggling, year, af...   \n",
       "\n",
       "                                       tagged_tokens  \n",
       "0  [ex_NN, wife_NN, threatening_VBG, suicide_NN, ...  \n",
       "1  [weird_JJ, not_RB, get_VB, affected_VBN, compl...  \n",
       "2  [finally_RB, almost_RB, never_RB, hear_VB, bad...  \n",
       "3       [need_NN, help_NN, help_NN, cry_NN, hard_RB]  \n",
       "4  [lost_VBN, hello_UH, name_NN, adam_NN, struggl...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbc2b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('pos_tagged_new_negation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2858d",
   "metadata": {},
   "source": [
    "### Combining All FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4fa64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.read_csv('pos_tagged_new_negation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d291a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd5b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df['tagged_tokens'] = pos_df['tagged_tokens'].apply(lambda x: [str(i) for i in ast.literal_eval(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76502efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>suicide</td>\n",
       "      <td>ex_NN wife_NN threatening_VBG suicide_NN recen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>weird_JJ not_RB get_VB affected_VBN compliment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>finally_RB almost_RB never_RB hear_VB bad_JJ y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide</td>\n",
       "      <td>need_NN help_NN help_NN cry_NN hard_RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide</td>\n",
       "      <td>lost_VBN hello_UH name_NN adam_NN struggling_V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class                                      tagged_tokens\n",
       "0      suicide  ex_NN wife_NN threatening_VBG suicide_NN recen...\n",
       "1  non-suicide  weird_JJ not_RB get_VB affected_VBN compliment...\n",
       "2  non-suicide  finally_RB almost_RB never_RB hear_VB bad_JJ y...\n",
       "3      suicide             need_NN help_NN help_NN cry_NN hard_RB\n",
       "4      suicide  lost_VBN hello_UH name_NN adam_NN struggling_V..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df.drop(columns = [\"text\",\"lemmatized_processed_text\",\"tokens\"], inplace = True)\n",
    "pos_df[\"tagged_tokens\"] = pos_df[\"tagged_tokens\"].apply(lambda x: \" \".join(x))\n",
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be754a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>stemmed_processed_text</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ex wife threatening suicide recently i left my...</td>\n",
       "      <td>1</td>\n",
       "      <td>['ex', 'wife', 'threaten', 'suicid', 'recent',...</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suicide', 'rece...</td>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>0</td>\n",
       "      <td>['weird', 'not', 'get', 'affect', 'compliment'...</td>\n",
       "      <td>['weird', 'not', 'get', 'affected', 'complimen...</td>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>finally is almost over so i can never hear has...</td>\n",
       "      <td>0</td>\n",
       "      <td>['final', 'almost', 'never', 'hear', 'bad', 'y...</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i need help just help me i am crying so hard</td>\n",
       "      <td>1</td>\n",
       "      <td>['need', 'help', 'help', 'cri', 'hard']</td>\n",
       "      <td>['need', 'help', 'help', 'cry', 'hard']</td>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i m so lost hello my name is adam and i ve bee...</td>\n",
       "      <td>1</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggl', '...</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggling'...</td>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  class  \\\n",
       "0           0  ex wife threatening suicide recently i left my...      1   \n",
       "1           1  am i weird i do not get affected by compliment...      0   \n",
       "2           2  finally is almost over so i can never hear has...      0   \n",
       "3           3       i need help just help me i am crying so hard      1   \n",
       "4           4  i m so lost hello my name is adam and i ve bee...      1   \n",
       "\n",
       "                              stemmed_processed_text  \\\n",
       "0  ['ex', 'wife', 'threaten', 'suicid', 'recent',...   \n",
       "1  ['weird', 'not', 'get', 'affect', 'compliment'...   \n",
       "2  ['final', 'almost', 'never', 'hear', 'bad', 'y...   \n",
       "3            ['need', 'help', 'help', 'cri', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggl', '...   \n",
       "\n",
       "                           lemmatized_processed_text  \\\n",
       "0  ['ex', 'wife', 'threatening', 'suicide', 'rece...   \n",
       "1  ['weird', 'not', 'get', 'affected', 'complimen...   \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3            ['need', 'help', 'help', 'cry', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggling'...   \n",
       "\n",
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation  \n",
       "0  0.078125      0.109375  \n",
       "1  0.133333      0.200000  \n",
       "2  0.100000      0.100000  \n",
       "3  0.000000      0.000000  \n",
       "4  0.042453      0.122642  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df = pd.read_csv(\"df_emotions.csv\")\n",
    "emotion_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5473d640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation  \n",
       "0  0.078125      0.109375  \n",
       "1  0.133333      0.200000  \n",
       "2  0.100000      0.100000  \n",
       "3  0.000000      0.000000  \n",
       "4  0.042453      0.122642  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df.drop(columns = [\"text\",\"lemmatized_processed_text\",\"class\",\"stemmed_processed_text\",\"Unnamed: 0\"],axis=1,inplace=True)\n",
    "emotion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe6a86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([emotion_df, pos_df], axis=1)\n",
    "combined_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b9e0798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>class</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>suicide</td>\n",
       "      <td>ex_NN wife_NN threatening_VBG suicide_NN recen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>weird_JJ not_RB get_VB affected_VBN compliment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>finally_RB almost_RB never_RB hear_VB bad_JJ y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>suicide</td>\n",
       "      <td>need_NN help_NN help_NN cry_NN hard_RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>suicide</td>\n",
       "      <td>lost_VBN hello_UH name_NN adam_NN struggling_V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation        class  \\\n",
       "0  0.078125      0.109375      suicide   \n",
       "1  0.133333      0.200000  non-suicide   \n",
       "2  0.100000      0.100000  non-suicide   \n",
       "3  0.000000      0.000000      suicide   \n",
       "4  0.042453      0.122642      suicide   \n",
       "\n",
       "                                       tagged_tokens  \n",
       "0  ex_NN wife_NN threatening_VBG suicide_NN recen...  \n",
       "1  weird_JJ not_RB get_VB affected_VBN compliment...  \n",
       "2  finally_RB almost_RB never_RB hear_VB bad_JJ y...  \n",
       "3             need_NN help_NN help_NN cry_NN hard_RB  \n",
       "4  lost_VBN hello_UH name_NN adam_NN struggling_V...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5852ad",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed3429",
   "metadata": {},
   "source": [
    "### POS_TAG + Domain_Dictionary Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bd91f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "##reading the domain dict from the text file\n",
    "with open(\"domain_dict_postag_lem.txt\", \"r\") as file:\n",
    "    list_of_lem = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee4070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_pos, test_X_pos, train_y_pos, test_y_pos = train_test_split(combined_df['tagged_tokens'],combined_df['class'],test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "080a436c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14499     teacher_NN partnered_VBN crush_NN science_NN c...\n",
       "32425     might_MD get_VB coronavirus_NN pretty_RB sure_...\n",
       "96329     feeling_NN closer_RBR closer_RBR edge_NN strug...\n",
       "173753    wish_NN end_NN dry_JJ cough_NN week_NN since_R...\n",
       "51707     everything_NN got_VBD plan_NN got_VBD goal_NN ...\n",
       "                                ...                        \n",
       "119934    case_NN forgot_VBD look_NN like_UH cutie_NN ng...\n",
       "103741    not_RB verge_NN pretty_RB well_UH hi_UH f_NN c...\n",
       "131989    wish_NN site_NN pro_JJ cut_NN like_UH one_CD d...\n",
       "146932    sm_NN ss_NNS mmm_NN aaa_NN mmm_NN mm_NN mmm_NN...\n",
       "122013    want_VB die_VB nearly_RB killed_VBN two_CD nig...\n",
       "Name: tagged_tokens, Length: 162385, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebc6d5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162385"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get all words from the document with pos_tag\n",
    "\n",
    "tagged_tokens_list = []\n",
    "for index, value in train_X_pos.items():\n",
    "    tagged_tokens_list.append(value)\n",
    "len(tagged_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c58669b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162385"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combining the document and dictionary\n",
    "tagged_tokens_list.extend(list_of_lem)\n",
    "len(tagged_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0595cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_pos = TfidfVectorizer()\n",
    "tfidf_vector_pos = tf_idf_pos.fit(tagged_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86e0014a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_tf_pos \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vector\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(train_X_pos)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#print dimension of data\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, n_features: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m X_train_tf_pos\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_vector' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_tf_pos = tfidf_vector.transform(train_X_pos)\n",
    "#print dimension of data\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train_tf_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming test data into tf-idf matrix\n",
    "X_test_tf_pos = tfidf_vector.transform(test_X_pos)\n",
    "\n",
    "#print dimension of data\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test_tf_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f16ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffa3a08d",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b06846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e80f4eb3",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81196ef",
   "metadata": {},
   "source": [
    "### POS_TAG + Domain_Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b21f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "naive_bayes_classifier_pos = MultinomialNB()\n",
    "naive_bayes_classifier_pos.fit(X_train_tf_pos, train_y_pos)\n",
    "#predicted y\n",
    "y_pred_nb_pos = naive_bayes_classifier_pos.predict(X_test_tf_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b058420",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y_pos, y_pred_nb_pos, target_names=['Non-Suicide', 'Suicide']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715ad751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_accuracy_pos = accuracy_score(y_pred_nb_pos, test_y_pos)\n",
    "print(\"Naive Bayes Accuracy Score -> \", nb_accuracy_pos * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Confusion Matrix\n",
    "print(\"Naive Bayes Confusion Matrix:\")\n",
    "print(metrics.confusion_matrix(test_y_pos, y_pred_nb_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_confusion_matrix_pos = metrics.confusion_matrix(test_y_pos, y_pred_nb_pos)\n",
    "\n",
    "nb_cm_display_pos = metrics.ConfusionMatrixDisplay(confusion_matrix = nb_confusion_matrix_pos, display_labels = [\"Non-Suicide\", \"Suicide\"])\n",
    "\n",
    "nb_cm_display_pos.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=test_y_pos)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c143eb66",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2b035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2caf1d87",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7885a0",
   "metadata": {},
   "source": [
    "### POS_TAG + Domain_Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the Logistic Regression classifier\n",
    "logreg_pos = LogisticRegression(max_iter=200)\n",
    "logreg_pos.fit(X_train_tf_pos,train_y_pos)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_logreg_pos = logreg_pos.predict(X_test_tf_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79881593",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y_pos, y_pred_logreg_pos, target_names=['Non-Suicide', 'Suicide']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5992fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_accuracy_pos = accuracy_score(y_pred_logreg_pos, test_y_pos)\n",
    "print(\"Logistic Regression Accuracy Score -> \", logreg_accuracy_pos * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5bb9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Reg Confusion Matrix\n",
    "print(\"Logistic Regression Confusion Matrix:\")\n",
    "print(metrics.confusion_matrix(test_y_pos, y_pred_logreg_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcadb43c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg_confusion_matrix_pos = metrics.confusion_matrix(test_y_pos, y_pred_logreg_pos)\n",
    "\n",
    "logreg_cm_display_pos = metrics.ConfusionMatrixDisplay(confusion_matrix = logreg_confusion_matrix_pos, display_labels = [\"Non-Suicide\", \"Suicide\"])\n",
    "\n",
    "logreg_cm_display_pos.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08963129",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e6b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "102c3085",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27953b4",
   "metadata": {},
   "source": [
    "### POS_TAG + Domain_Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21efbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the SVM classifier\n",
    "linearSVC_pos = LinearSVC()\n",
    "linearSVC_pos.fit(X_train_tf_pos,train_y_pos)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_SVC_pos = linearSVC_pos.predict(X_test_tf_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d3952",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y_pos, y_pred_SVC_pos, target_names=['Non-Suicide', 'Suicide']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30722d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_accuracy_pos = accuracy_score(y_pred_SVC_pos, test_y_pos)\n",
    "print(\"SVM Accuracy Score -> \",svc_accuracy_pos*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2826e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Confusion Matrix\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "print(metrics.confusion_matrix(test_y_pos, y_pred_SVC_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa687d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc_confusion_matrix_pos = metrics.confusion_matrix(test_y_pos, y_pred_SVC_pos)\n",
    "\n",
    "svc_cm_display_pos = metrics.ConfusionMatrixDisplay(confusion_matrix = svc_confusion_matrix_pos, display_labels = [\"Non-Suicide\", \"Suicide\"])\n",
    "\n",
    "svc_cm_display_pos.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba53be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2acafbdb",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e91df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
