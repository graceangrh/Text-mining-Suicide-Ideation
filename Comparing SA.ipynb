{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc80ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import ast\n",
    "from nrclex import NRCLex\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import loguniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee51233e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>stemmed_processed_text</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ex wife threatening suicide recently i left my...</td>\n",
       "      <td>1</td>\n",
       "      <td>['ex', 'wife', 'threaten', 'suicid', 'recent',...</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suicide', 'rece...</td>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>0</td>\n",
       "      <td>['weird', 'not', 'get', 'affect', 'compliment'...</td>\n",
       "      <td>['weird', 'not', 'get', 'affected', 'complimen...</td>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>finally is almost over so i can never hear has...</td>\n",
       "      <td>0</td>\n",
       "      <td>['final', 'almost', 'never', 'hear', 'bad', 'y...</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i need help just help me i am crying so hard</td>\n",
       "      <td>1</td>\n",
       "      <td>['need', 'help', 'help', 'cri', 'hard']</td>\n",
       "      <td>['need', 'help', 'help', 'cry', 'hard']</td>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i m so lost hello my name is adam and i ve bee...</td>\n",
       "      <td>1</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggl', '...</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggling'...</td>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  class  \\\n",
       "0           0  ex wife threatening suicide recently i left my...      1   \n",
       "1           1  am i weird i do not get affected by compliment...      0   \n",
       "2           2  finally is almost over so i can never hear has...      0   \n",
       "3           3       i need help just help me i am crying so hard      1   \n",
       "4           4  i m so lost hello my name is adam and i ve bee...      1   \n",
       "\n",
       "                              stemmed_processed_text  \\\n",
       "0  ['ex', 'wife', 'threaten', 'suicid', 'recent',...   \n",
       "1  ['weird', 'not', 'get', 'affect', 'compliment'...   \n",
       "2  ['final', 'almost', 'never', 'hear', 'bad', 'y...   \n",
       "3            ['need', 'help', 'help', 'cri', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggl', '...   \n",
       "\n",
       "                           lemmatized_processed_text  \\\n",
       "0  ['ex', 'wife', 'threatening', 'suicide', 'rece...   \n",
       "1  ['weird', 'not', 'get', 'affected', 'complimen...   \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3            ['need', 'help', 'help', 'cry', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggling'...   \n",
       "\n",
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation  \n",
       "0  0.078125      0.109375  \n",
       "1  0.133333      0.200000  \n",
       "2  0.100000      0.100000  \n",
       "3  0.000000      0.000000  \n",
       "4  0.042453      0.122642  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df = pd.read_csv(\"df_emotions.csv\")\n",
    "emotion_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f956ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation  \n",
       "0  0.078125      0.109375  \n",
       "1  0.133333      0.200000  \n",
       "2  0.100000      0.100000  \n",
       "3  0.000000      0.000000  \n",
       "4  0.042453      0.122642  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df.drop(columns = [\"text\",\"lemmatized_processed_text\",\"class\",\"stemmed_processed_text\",\"Unnamed: 0\"],axis=1,inplace=True)\n",
    "emotion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c6b468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>stemmed_processed_text</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ex wife threatening suicide recently i left my...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threaten', 'suicid', 'recent',...</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suicide', 'rece...</td>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.9655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'not', 'get', 'affect', 'compliment'...</td>\n",
       "      <td>['weird', 'not', 'get', 'affected', 'complimen...</td>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.0984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>finally is almost over so i can never hear has...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['final', 'almost', 'never', 'hear', 'bad', 'y...</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i need help just help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'help', 'help', 'cri', 'hard']</td>\n",
       "      <td>['need', 'help', 'help', 'cry', 'hard']</td>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i m so lost hello my name is adam and i ve bee...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggl', '...</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggling'...</td>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.9965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text        class  \\\n",
       "0           0  ex wife threatening suicide recently i left my...      suicide   \n",
       "1           1  am i weird i do not get affected by compliment...  non-suicide   \n",
       "2           2  finally is almost over so i can never hear has...  non-suicide   \n",
       "3           3       i need help just help me i am crying so hard      suicide   \n",
       "4           4  i m so lost hello my name is adam and i ve bee...      suicide   \n",
       "\n",
       "                              stemmed_processed_text  \\\n",
       "0  ['ex', 'wife', 'threaten', 'suicid', 'recent',...   \n",
       "1  ['weird', 'not', 'get', 'affect', 'compliment'...   \n",
       "2  ['final', 'almost', 'never', 'hear', 'bad', 'y...   \n",
       "3            ['need', 'help', 'help', 'cri', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggl', '...   \n",
       "\n",
       "                           lemmatized_processed_text  \\\n",
       "0  ['ex', 'wife', 'threatening', 'suicide', 'rece...   \n",
       "1  ['weird', 'not', 'get', 'affected', 'complimen...   \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3            ['need', 'help', 'help', 'cry', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggling'...   \n",
       "\n",
       "                     clean_lemmatized_processed_text    neg    neu    pos  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.378  0.437  0.186   \n",
       "1  weird not get affected compliment coming someo...  0.225  0.529  0.245   \n",
       "2  finally almost never hear bad year ever swear ...  0.259  0.433  0.308   \n",
       "3                            need help help cry hard  0.413  0.092  0.495   \n",
       "4  lost hello name adam struggling year afraid pa...  0.368  0.505  0.127   \n",
       "\n",
       "   compound  \n",
       "0   -0.9655  \n",
       "1    0.0984  \n",
       "2    0.2025  \n",
       "3    0.2263  \n",
       "4   -0.9965  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df_2 = pd.read_csv(\"lexicon_approach2.csv\")\n",
    "emotion_df_2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "065a9563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>suicide</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.9655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.0984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.9965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class    neg    neu    pos  compound\n",
       "0      suicide  0.378  0.437  0.186   -0.9655\n",
       "1  non-suicide  0.225  0.529  0.245    0.0984\n",
       "2  non-suicide  0.259  0.433  0.308    0.2025\n",
       "3      suicide  0.413  0.092  0.495    0.2263\n",
       "4      suicide  0.368  0.505  0.127   -0.9965"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df_2.drop(columns = [\"text\",\"lemmatized_processed_text\",\"stemmed_processed_text\",\"Unnamed: 0\",\"clean_lemmatized_processed_text\"],axis=1,inplace=True)\n",
    "emotion_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e07fea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>class</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.9655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.0984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.9965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation        class    neg    neu    pos  compound  \n",
       "0  0.078125      0.109375      suicide  0.378  0.437  0.186   -0.9655  \n",
       "1  0.133333      0.200000  non-suicide  0.225  0.529  0.245    0.0984  \n",
       "2  0.100000      0.100000  non-suicide  0.259  0.433  0.308    0.2025  \n",
       "3  0.000000      0.000000      suicide  0.413  0.092  0.495    0.2263  \n",
       "4  0.042453      0.122642      suicide  0.368  0.505  0.127   -0.9965  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.concat([emotion_df, emotion_df_2], axis=1)\n",
    "combined_df.dropna(inplace=True)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bf9b6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>class</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>norm_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.9655</td>\n",
       "      <td>0.01725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.0984</td>\n",
       "      <td>0.54920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.2025</td>\n",
       "      <td>0.60125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>0.61315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.9965</td>\n",
       "      <td>0.00175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation        class    neg    neu    pos  compound  \\\n",
       "0  0.078125      0.109375      suicide  0.378  0.437  0.186   -0.9655   \n",
       "1  0.133333      0.200000  non-suicide  0.225  0.529  0.245    0.0984   \n",
       "2  0.100000      0.100000  non-suicide  0.259  0.433  0.308    0.2025   \n",
       "3  0.000000      0.000000      suicide  0.413  0.092  0.495    0.2263   \n",
       "4  0.042453      0.122642      suicide  0.368  0.505  0.127   -0.9965   \n",
       "\n",
       "   norm_compound  \n",
       "0        0.01725  \n",
       "1        0.54920  \n",
       "2        0.60125  \n",
       "3        0.61315  \n",
       "4        0.00175  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Normalizing Data as naive bayes dont accept negative\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "combined_df['norm_compound'] = scaler.fit_transform(combined_df[['compound']])\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19062412",
   "metadata": {},
   "source": [
    "### TESTING 3 VECTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e28c50d",
   "metadata": {},
   "source": [
    "#### NRC FLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953247dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "# Create a sparse matrix for the extra features\n",
    "extra_features_sparse = sp.csr_matrix(combined_df[[\"fear\", \"anger\", \"anticip\", \"trust\", \"surprise\", \"positive\", \"negative\", \"sadness\",\"disgust\", \"joy\", \"anticipation\"]].values)\n",
    "\n",
    "# Fit and transform the TF-IDF matrix\n",
    "tfidf_vectorizer_sa_nrc = TfidfVectorizer()\n",
    "tfidf_sa_nrc = tfidf_vectorizer_sa_nrc.fit_transform(combined_df.clean_lemmatized_processed_text)\n",
    "\n",
    "# Combine the two sparse matrices using hstack\n",
    "all_features_sa_nrc = sp.hstack([tfidf_sa_nrc, extra_features_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0736c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_sa_nrc, test_X_sa_nrc, train_y_sa_nrc, test_y_sa_nrc = train_test_split(all_features_sa_nrc,combined_df['class'],test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d73df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 162385, n_features: 47565\n",
      "n_samples: 69594, n_features: 47565\n"
     ]
    }
   ],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % train_X_sa_nrc.shape)\n",
    "print(\"n_samples: %d, n_features: %d\" % test_X_sa_nrc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76953aa3",
   "metadata": {},
   "source": [
    "#### VADER1  (Pos, neg, neu, comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45fd94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features_sparse = sp.csr_matrix(combined_df[[\"pos\",\"neg\",\"neu\",\"norm_compound\"]].values)\n",
    "\n",
    "# Fit and transform the TF-IDF matrix\n",
    "tfidf_vectorizer_sa_va_1 = TfidfVectorizer()\n",
    "tfidf_sa_va_1 = tfidf_vectorizer_sa_va_1.fit_transform(combined_df.clean_lemmatized_processed_text)\n",
    "\n",
    "# Combine the two sparse matrices using hstack\n",
    "all_features_sa_va_1 = sp.hstack([tfidf_sa_va_1, extra_features_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a1e6e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_sa_va_1, test_X_sa_va_1, train_y_sa_va_1, test_y_sa_va_1 = train_test_split(all_features_sa_va_1,combined_df['class'],test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d35c102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 162385, n_features: 47558\n",
      "n_samples: 69594, n_features: 47558\n"
     ]
    }
   ],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % train_X_sa_va_1.shape)\n",
    "print(\"n_samples: %d, n_features: %d\" % test_X_sa_va_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e89df",
   "metadata": {},
   "source": [
    "#### VADER2  (comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e15aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features_sparse = sp.csr_matrix(combined_df[[\"norm_compound\"]].values)\n",
    "\n",
    "# Fit and transform the TF-IDF matrix\n",
    "tfidf_vectorizer_sa_va_2 = TfidfVectorizer()\n",
    "tfidf_sa_va_2 = tfidf_vectorizer_sa_va_2.fit_transform(combined_df.clean_lemmatized_processed_text)\n",
    "\n",
    "# Combine the two sparse matrices using hstack\n",
    "all_features_sa_va_2 = sp.hstack([tfidf_sa_va_2, extra_features_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "327a2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_sa_va_2, test_X_sa_va_2, train_y_sa_va_2, test_y_sa_va_2 = train_test_split(all_features_sa_va_2,combined_df['class'],test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "077ed783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 162385, n_features: 47555\n",
      "n_samples: 69594, n_features: 47555\n"
     ]
    }
   ],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % train_X_sa_va_2.shape)\n",
    "print(\"n_samples: %d, n_features: %d\" % test_X_sa_va_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5398ddfd",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc2d63",
   "metadata": {},
   "source": [
    "#### NRC FLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "844cc482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "naive_bayes_classifier_sa_nrc = MultinomialNB()\n",
    "naive_bayes_classifier_sa_nrc.fit(train_X_sa_nrc, train_y_sa_nrc)\n",
    "#predicted y\n",
    "y_pred_nb_sa_nrc = naive_bayes_classifier_sa_nrc.predict(test_X_sa_nrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9db208b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  89.67008650170992\n"
     ]
    }
   ],
   "source": [
    "nb_accuracy_sa_nrc = accuracy_score(y_pred_nb_sa_nrc, test_y_sa_nrc)\n",
    "print(\"Naive Bayes Accuracy Score -> \", nb_accuracy_sa_nrc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc349012",
   "metadata": {},
   "source": [
    "#### VADER1  (Pos, neg, neu, comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a63a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "naive_bayes_classifier_sa_va_1 = MultinomialNB()\n",
    "naive_bayes_classifier_sa_va_1.fit(train_X_sa_va_1, train_y_sa_va_1)\n",
    "#predicted y\n",
    "y_pred_nb_sa_va_1 = naive_bayes_classifier_sa_va_1.predict(test_X_sa_va_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc191d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  90.46613213782797\n"
     ]
    }
   ],
   "source": [
    "nb_accuracy_sa_va_1 = accuracy_score(y_pred_nb_sa_va_1, test_y_sa_va_1)\n",
    "print(\"Naive Bayes Accuracy Score -> \", nb_accuracy_sa_va_1 * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc953540",
   "metadata": {},
   "source": [
    "#### VADER1  (comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dac7b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "naive_bayes_classifier_sa_va_2 = MultinomialNB()\n",
    "naive_bayes_classifier_sa_va_2.fit(train_X_sa_va_2, train_y_sa_va_2)\n",
    "#predicted y\n",
    "y_pred_nb_sa_va_2 = naive_bayes_classifier_sa_va_2.predict(test_X_sa_va_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e5c5331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  90.08535218553324\n"
     ]
    }
   ],
   "source": [
    "nb_accuracy_sa_va_2 = accuracy_score(y_pred_nb_sa_va_2, test_y_sa_va_2)\n",
    "print(\"Naive Bayes Accuracy Score -> \", nb_accuracy_sa_va_2 * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4323271a",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8faab",
   "metadata": {},
   "source": [
    "#### NRC FLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd4acb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the Logistic Regression classifier\n",
    "logreg_sa_nrc = LogisticRegression(max_iter=200)\n",
    "logreg_sa_nrc.fit(train_X_sa_nrc,train_y_sa_nrc)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_logreg_sa_nrc = logreg_sa_nrc.predict(test_X_sa_nrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b60d2e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score ->  93.52961462195016\n"
     ]
    }
   ],
   "source": [
    "logreg_accuracy_sa_nrc = accuracy_score(y_pred_logreg_sa_nrc, test_y_sa_nrc)\n",
    "print(\"Logistic Regression Accuracy Score -> \", logreg_accuracy_sa_nrc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6c4ca",
   "metadata": {},
   "source": [
    "#### VADER1  (Pos, neg, neu, comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d045071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the Logistic Regression classifier\n",
    "logreg_sa_va_1 = LogisticRegression(max_iter=200)\n",
    "logreg_sa_va_1.fit(train_X_sa_va_1,train_y_sa_va_1)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_logreg_sa_va_1 = logreg_sa_va_1.predict(test_X_sa_va_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c83eaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score ->  93.54685748771446\n"
     ]
    }
   ],
   "source": [
    "logreg_accuracy_sa_va_1 = accuracy_score(y_pred_logreg_sa_va_1, test_y_sa_va_1)\n",
    "print(\"Logistic Regression Accuracy Score -> \", logreg_accuracy_sa_va_1 * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cf536",
   "metadata": {},
   "source": [
    "#### VADER1  (comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36c86493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the Logistic Regression classifier\n",
    "logreg_sa_va_2 = LogisticRegression(max_iter=200)\n",
    "logreg_sa_va_2.fit(train_X_sa_va_2,train_y_sa_va_2)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_logreg_sa_va_2 = logreg_sa_va_2.predict(test_X_sa_va_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aef47c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score ->  93.52961462195016\n"
     ]
    }
   ],
   "source": [
    "logreg_accuracy_sa_va_2 = accuracy_score(y_pred_logreg_sa_va_2, test_y_sa_va_2)\n",
    "print(\"Logistic Regression Accuracy Score -> \", logreg_accuracy_sa_va_2 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d648e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00eb44f3",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768bfdcb",
   "metadata": {},
   "source": [
    "#### NRC FLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d5607a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the SVM classifier\n",
    "linearSVC_sa_nrc = LinearSVC()\n",
    "linearSVC_sa_nrc.fit(train_X_sa_nrc,train_y_sa_nrc)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_SVC_sa_nrc = linearSVC_sa_nrc.predict(test_X_sa_nrc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2eb1a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  93.45489553697158\n"
     ]
    }
   ],
   "source": [
    "svc_accuracy_sa_nrc = accuracy_score(y_pred_SVC_sa_nrc, test_y_sa_nrc)\n",
    "print(\"SVM Accuracy Score -> \",svc_accuracy_sa_nrc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc221aef",
   "metadata": {},
   "source": [
    "#### VADER1  (Pos, neg, neu, comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f7dc502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the SVM classifier\n",
    "linearSVC_sa_va_1 = LinearSVC()\n",
    "linearSVC_sa_va_1.fit(train_X_sa_va_1,train_y_sa_va_1)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_SVC_sa_va_1 = linearSVC_sa_va_1.predict(test_X_sa_va_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a070e4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  93.47070149725552\n"
     ]
    }
   ],
   "source": [
    "svc_accuracy_sa_va_1 = accuracy_score(y_pred_SVC_sa_va_1, test_y_sa_va_1)\n",
    "print(\"SVM Accuracy Score -> \",svc_accuracy_sa_va_1*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3c1eb",
   "metadata": {},
   "source": [
    "#### VADER1  (comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f26505c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the SVM classifier\n",
    "linearSVC_sa_va_2 = LinearSVC()\n",
    "linearSVC_sa_va_2.fit(train_X_sa_va_2,train_y_sa_va_2)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_SVC_sa_va_2 = linearSVC_sa_va_2.predict(test_X_sa_va_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb7335f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  93.4678276862948\n"
     ]
    }
   ],
   "source": [
    "svc_accuracy_sa_va_2 = accuracy_score(y_pred_SVC_sa_va_2, test_y_sa_va_2)\n",
    "print(\"SVM Accuracy Score -> \",svc_accuracy_sa_va_2*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62305c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
