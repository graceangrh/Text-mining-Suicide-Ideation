{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc80ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import ast\n",
    "from nrclex import NRCLex\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score, fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee51233e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>stemmed_processed_text</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ex wife threatening suicide recently i left my...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threaten', 'suicid', 'recent',...</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suicide', 'rece...</td>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'not', 'get', 'affect', 'compliment'...</td>\n",
       "      <td>['weird', 'not', 'get', 'affected', 'complimen...</td>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>finally is almost over so i can never hear has...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['final', 'almost', 'never', 'hear', 'bad', 'y...</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i need help just help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'help', 'help', 'cri', 'hard']</td>\n",
       "      <td>['need', 'help', 'help', 'cry', 'hard']</td>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i m so lost hello my name is adam and i ve bee...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggl', '...</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggling'...</td>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text        class  \\\n",
       "0           0  ex wife threatening suicide recently i left my...      suicide   \n",
       "1           1  am i weird i do not get affected by compliment...  non-suicide   \n",
       "2           2  finally is almost over so i can never hear has...  non-suicide   \n",
       "3           3       i need help just help me i am crying so hard      suicide   \n",
       "4           4  i m so lost hello my name is adam and i ve bee...      suicide   \n",
       "\n",
       "                              stemmed_processed_text  \\\n",
       "0  ['ex', 'wife', 'threaten', 'suicid', 'recent',...   \n",
       "1  ['weird', 'not', 'get', 'affect', 'compliment'...   \n",
       "2  ['final', 'almost', 'never', 'hear', 'bad', 'y...   \n",
       "3            ['need', 'help', 'help', 'cri', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggl', '...   \n",
       "\n",
       "                           lemmatized_processed_text  \\\n",
       "0  ['ex', 'wife', 'threatening', 'suicide', 'rece...   \n",
       "1  ['weird', 'not', 'get', 'affected', 'complimen...   \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3            ['need', 'help', 'help', 'cry', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggling'...   \n",
       "\n",
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation  \n",
       "0  0.078125      0.109375  \n",
       "1  0.133333      0.200000  \n",
       "2  0.100000      0.100000  \n",
       "3  0.000000      0.000000  \n",
       "4  0.042453      0.122642  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df = pd.read_csv(\"df_emotions.csv\")\n",
    "emotion_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f956ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation  \n",
       "0  0.078125      0.109375  \n",
       "1  0.133333      0.200000  \n",
       "2  0.100000      0.100000  \n",
       "3  0.000000      0.000000  \n",
       "4  0.042453      0.122642  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df.drop(columns = [\"text\",\"lemmatized_processed_text\",\"class\",\"stemmed_processed_text\",\"Unnamed: 0\"],axis=1,inplace=True)\n",
    "emotion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c6b468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>stemmed_processed_text</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ex wife threatening suicide recently i left my...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threaten', 'suicid', 'recent',...</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suicide', 'rece...</td>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.9655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'not', 'get', 'affect', 'compliment'...</td>\n",
       "      <td>['weird', 'not', 'get', 'affected', 'complimen...</td>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.0984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>finally is almost over so i can never hear has...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['final', 'almost', 'never', 'hear', 'bad', 'y...</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i need help just help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'help', 'help', 'cri', 'hard']</td>\n",
       "      <td>['need', 'help', 'help', 'cry', 'hard']</td>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i m so lost hello my name is adam and i ve bee...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggl', '...</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggling'...</td>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.9965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text        class  \\\n",
       "0           0  ex wife threatening suicide recently i left my...      suicide   \n",
       "1           1  am i weird i do not get affected by compliment...  non-suicide   \n",
       "2           2  finally is almost over so i can never hear has...  non-suicide   \n",
       "3           3       i need help just help me i am crying so hard      suicide   \n",
       "4           4  i m so lost hello my name is adam and i ve bee...      suicide   \n",
       "\n",
       "                              stemmed_processed_text  \\\n",
       "0  ['ex', 'wife', 'threaten', 'suicid', 'recent',...   \n",
       "1  ['weird', 'not', 'get', 'affect', 'compliment'...   \n",
       "2  ['final', 'almost', 'never', 'hear', 'bad', 'y...   \n",
       "3            ['need', 'help', 'help', 'cri', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggl', '...   \n",
       "\n",
       "                           lemmatized_processed_text  \\\n",
       "0  ['ex', 'wife', 'threatening', 'suicide', 'rece...   \n",
       "1  ['weird', 'not', 'get', 'affected', 'complimen...   \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3            ['need', 'help', 'help', 'cry', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggling'...   \n",
       "\n",
       "                     clean_lemmatized_processed_text    neg    neu    pos  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.378  0.437  0.186   \n",
       "1  weird not get affected compliment coming someo...  0.225  0.529  0.245   \n",
       "2  finally almost never hear bad year ever swear ...  0.259  0.433  0.308   \n",
       "3                            need help help cry hard  0.413  0.092  0.495   \n",
       "4  lost hello name adam struggling year afraid pa...  0.368  0.505  0.127   \n",
       "\n",
       "   compound  \n",
       "0   -0.9655  \n",
       "1    0.0984  \n",
       "2    0.2025  \n",
       "3    0.2263  \n",
       "4   -0.9965  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df_2 = pd.read_csv(\"lexicon_approach2.csv\")\n",
    "emotion_df_2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "065a9563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>suicide</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.9655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.0984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.9965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class    neg    neu    pos  compound\n",
       "0      suicide  0.378  0.437  0.186   -0.9655\n",
       "1  non-suicide  0.225  0.529  0.245    0.0984\n",
       "2  non-suicide  0.259  0.433  0.308    0.2025\n",
       "3      suicide  0.413  0.092  0.495    0.2263\n",
       "4      suicide  0.368  0.505  0.127   -0.9965"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df_2.drop(columns = [\"text\",\"lemmatized_processed_text\",\"stemmed_processed_text\",\"Unnamed: 0\",\"clean_lemmatized_processed_text\"],axis=1,inplace=True)\n",
    "emotion_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd8fddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex_NN wife_NN threatening_VBG suicide_NN recen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weird_JJ not_RB get_VB affected_VBN compliment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally_RB almost_RB never_RB hear_VB bad_JJ y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need_NN help_NN help_NN cry_NN hard_RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lost_VBN hello_UH name_NN adam_NN struggling_V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tagged_tokens\n",
       "0  ex_NN wife_NN threatening_VBG suicide_NN recen...\n",
       "1  weird_JJ not_RB get_VB affected_VBN compliment...\n",
       "2  finally_RB almost_RB never_RB hear_VB bad_JJ y...\n",
       "3             need_NN help_NN help_NN cry_NN hard_RB\n",
       "4  lost_VBN hello_UH name_NN adam_NN struggling_V..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####get POS_tagged\n",
    "pos_df = pd.read_csv('pos_tagged_new_negation.csv')\n",
    "\n",
    "pos_df.dropna(inplace=True)\n",
    "\n",
    "pos_df['tagged_tokens'] = pos_df['tagged_tokens'].apply(lambda x: [str(i) for i in ast.literal_eval(x)])\n",
    "\n",
    "pos_df.drop(columns = [\"text\",\"lemmatized_processed_text\",\"tokens\",\"class\"], inplace = True)\n",
    "pos_df[\"tagged_tokens\"] = pos_df[\"tagged_tokens\"].apply(lambda x: \" \".join(x))\n",
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e07fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([emotion_df, emotion_df_2,pos_df ], axis=1)\n",
    "combined_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf9b6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>class</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>tagged_tokens</th>\n",
       "      <th>norm_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.9655</td>\n",
       "      <td>ex_NN wife_NN threatening_VBG suicide_NN recen...</td>\n",
       "      <td>0.01725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.0984</td>\n",
       "      <td>weird_JJ not_RB get_VB affected_VBN compliment...</td>\n",
       "      <td>0.54920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.2025</td>\n",
       "      <td>finally_RB almost_RB never_RB hear_VB bad_JJ y...</td>\n",
       "      <td>0.60125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>need_NN help_NN help_NN cry_NN hard_RB</td>\n",
       "      <td>0.61315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.9965</td>\n",
       "      <td>lost_VBN hello_UH name_NN adam_NN struggling_V...</td>\n",
       "      <td>0.00175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation        class    neg    neu    pos  compound  \\\n",
       "0  0.078125      0.109375      suicide  0.378  0.437  0.186   -0.9655   \n",
       "1  0.133333      0.200000  non-suicide  0.225  0.529  0.245    0.0984   \n",
       "2  0.100000      0.100000  non-suicide  0.259  0.433  0.308    0.2025   \n",
       "3  0.000000      0.000000      suicide  0.413  0.092  0.495    0.2263   \n",
       "4  0.042453      0.122642      suicide  0.368  0.505  0.127   -0.9965   \n",
       "\n",
       "                                       tagged_tokens  norm_compound  \n",
       "0  ex_NN wife_NN threatening_VBG suicide_NN recen...        0.01725  \n",
       "1  weird_JJ not_RB get_VB affected_VBN compliment...        0.54920  \n",
       "2  finally_RB almost_RB never_RB hear_VB bad_JJ y...        0.60125  \n",
       "3             need_NN help_NN help_NN cry_NN hard_RB        0.61315  \n",
       "4  lost_VBN hello_UH name_NN adam_NN struggling_V...        0.00175  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Normalizing Data as naive bayes dont accept negative\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "combined_df['norm_compound'] = scaler.fit_transform(combined_df[['compound']])\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15aae198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_test, y_pred):\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(conf_matrix)\n",
    "    TP = conf_matrix[0][0]\n",
    "    FN = conf_matrix[1][0]\n",
    "    FP = conf_matrix[0][1]\n",
    "    TN = conf_matrix[1][1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    precision = precision_score(y_test, y_pred)*100\n",
    "    recall = recall_score(y_test, y_pred)*100\n",
    "\n",
    "    print('TP:',TP); print('FN:',FN); print('FP:',FP) ;print('TN:',TN)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "\n",
    "    f1_score = 2*((precision * recall) / ((precision + recall)))\n",
    "    print('F1 Score:', f1_score)\n",
    "\n",
    "    f2score = ((1 + 2**2) * precision * recall) / (2**2 * precision + recall)\n",
    "    print('F2 Score:', f2score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19062412",
   "metadata": {},
   "source": [
    "### TESTING 3 VECTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e28c50d",
   "metadata": {},
   "source": [
    "#### NRC FLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "995e83a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##reading the domain dict from the text file\n",
    "with open(\"domain_dict_postag_lem.txt\", \"r\") as file:\n",
    "    list_of_lem = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe1d5892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231979"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tokens_list = []\n",
    "for index, value in combined_df['tagged_tokens'].items():\n",
    "    tagged_tokens_list.append(value)\n",
    "len(tagged_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac25b564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231979"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tokens_list.extend(list_of_lem)\n",
    "len(tagged_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "953247dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "# Create a sparse matrix for the extra features\n",
    "extra_features_sparse = sp.csr_matrix(combined_df[[\"fear\", \"anger\", \"anticip\", \"trust\", \"surprise\", \"positive\", \"negative\", \"sadness\",\"disgust\", \"joy\", \"anticipation\"]].values)\n",
    "\n",
    "# Fit and transform the TF-IDF matrix\n",
    "tfidf_vectorizer_sa_nrc = TfidfVectorizer()\n",
    "tfidf_sa_nrc = tfidf_vectorizer_sa_nrc.fit_transform(tagged_tokens_list)\n",
    "\n",
    "# Combine the two sparse matrices using hstack\n",
    "all_features_sa_nrc = sp.hstack([tfidf_sa_nrc, extra_features_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0736c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_sa_nrc, test_X_sa_nrc, train_y_sa_nrc, test_y_sa_nrc = train_test_split(all_features_sa_nrc,combined_df['class'],test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d73df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 162385, n_features: 47588\n",
      "n_samples: 69594, n_features: 47588\n"
     ]
    }
   ],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % train_X_sa_nrc.shape)\n",
    "print(\"n_samples: %d, n_features: %d\" % test_X_sa_nrc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ea9c41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162385, 47588)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_sa_nrc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bf4b620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208365    non-suicide\n",
       "49348         suicide\n",
       "163403    non-suicide\n",
       "116631    non-suicide\n",
       "149784    non-suicide\n",
       "             ...     \n",
       "139580        suicide\n",
       "128493    non-suicide\n",
       "185478    non-suicide\n",
       "214175    non-suicide\n",
       "212553    non-suicide\n",
       "Name: class, Length: 69594, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sa_nrc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76953aa3",
   "metadata": {},
   "source": [
    "#### VADER1  (Pos, neg, neu, comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45fd94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features_sparse = sp.csr_matrix(combined_df[[\"pos\",\"neg\",\"neu\",\"norm_compound\"]].values)\n",
    "\n",
    "# Fit and transform the TF-IDF matrix\n",
    "tfidf_vectorizer_sa_va_1 = TfidfVectorizer()\n",
    "tfidf_sa_va_1 = tfidf_vectorizer_sa_va_1.fit_transform(tagged_tokens_list)\n",
    "\n",
    "# Combine the two sparse matrices using hstack\n",
    "all_features_sa_va_1 = sp.hstack([tfidf_sa_va_1, extra_features_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a1e6e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_sa_va_1, test_X_sa_va_1, train_y_sa_va_1, test_y_sa_va_1 = train_test_split(all_features_sa_va_1,combined_df['class'],test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d35c102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 162385, n_features: 47581\n",
      "n_samples: 69594, n_features: 47581\n"
     ]
    }
   ],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % train_X_sa_va_1.shape)\n",
    "print(\"n_samples: %d, n_features: %d\" % test_X_sa_va_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e89df",
   "metadata": {},
   "source": [
    "#### VADER2  (comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features_sparse = sp.csr_matrix(combined_df[[\"norm_compound\"]].values)\n",
    "\n",
    "# Fit and transform the TF-IDF matrix\n",
    "tfidf_vectorizer_sa_va_2 = TfidfVectorizer()\n",
    "tfidf_sa_va_2 = tfidf_vectorizer_sa_va_2.fit_transform(tagged_tokens_list)\n",
    "\n",
    "# Combine the two sparse matrices using hstack\n",
    "all_features_sa_va_2 = sp.hstack([tfidf_sa_va_2, extra_features_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_sa_va_2, test_X_sa_va_2, train_y_sa_va_2, test_y_sa_va_2 = train_test_split(all_features_sa_va_2,combined_df['class'],test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ed783",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % train_X_sa_va_2.shape)\n",
    "print(\"n_samples: %d, n_features: %d\" % test_X_sa_va_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5398ddfd",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc2d63",
   "metadata": {},
   "source": [
    "#### NRC LEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844cc482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "naive_bayes_classifier_sa_nrc = MultinomialNB()\n",
    "naive_bayes_classifier_sa_nrc.fit(train_X_sa_nrc, train_y_sa_nrc)\n",
    "#predicted y\n",
    "y_pred_nb_sa_nrc = naive_bayes_classifier_sa_nrc.predict(test_X_sa_nrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc191d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_accuracy_sa_nrc = accuracy_score(y_pred_nb_sa_nrc, test_y_sa_nrc)\n",
    "print(\"Naive Bayes Accuracy Score -> \", nb_accuracy_sa_nrc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y_sa_nrc, y_pred_nb_sa_nrc, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3808ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(test_y_sa_nrc, y_pred_nb_sa_nrc)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc349012",
   "metadata": {},
   "source": [
    "#### VADER1  (Pos, neg, neu, comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a63a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "naive_bayes_classifier_sa_va_1 = MultinomialNB()\n",
    "naive_bayes_classifier_sa_va_1.fit(train_X_sa_va_1, train_y_sa_va_1)\n",
    "#predicted y\n",
    "y_pred_nb_sa_va_1 = naive_bayes_classifier_sa_va_1.predict(test_X_sa_va_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb612ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_accuracy_sa_va_1 = accuracy_score(y_pred_nb_sa_va_1, test_y_sa_va_1)\n",
    "print(\"Naive Bayes Accuracy Score -> \", nb_accuracy_sa_va_1 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(test_y_sa_va_1, y_pred_nb_sa_va_1, average='macro')\n",
    "print(\"Recall score:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d380216",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y_sa_va_1, y_pred_nb_sa_va_1, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da5e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(test_y_sa_va_1, y_pred_nb_sa_va_1)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc953540",
   "metadata": {},
   "source": [
    "#### VADER1  (comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "naive_bayes_classifier_sa_va_2 = MultinomialNB()\n",
    "naive_bayes_classifier_sa_va_2.fit(train_X_sa_va_2, train_y_sa_va_2)\n",
    "#predicted y\n",
    "y_pred_nb_sa_va_2 = naive_bayes_classifier_sa_va_2.predict(test_X_sa_va_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_accuracy_sa_va_2 = accuracy_score(y_pred_nb_sa_va_2, test_y_sa_va_2)\n",
    "print(\"Naive Bayes Accuracy Score -> \", nb_accuracy_sa_va_2 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y_sa_va_2, y_pred_nb_sa_va_2, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4323271a",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8faab",
   "metadata": {},
   "source": [
    "#### NRC LEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4acb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the Logistic Regression classifier\n",
    "logreg_sa_nrc = LogisticRegression(max_iter=200)\n",
    "logreg_sa_nrc.fit(train_X_sa_nrc,train_y_sa_nrc)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_logreg_sa_nrc = logreg_sa_nrc.predict(test_X_sa_nrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_accuracy_sa_nrc = accuracy_score(y_pred_logreg_sa_nrc, test_y_sa_nrc)\n",
    "print(\"Logistic Regression Accuracy Score -> \", logreg_accuracy_sa_nrc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363171ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y_sa_nrc, y_pred_logreg_sa_nrc, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7dd5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(test_y_sa_nrc, y_pred_logreg_sa_nrc)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6c4ca",
   "metadata": {},
   "source": [
    "#### VADER1  (Pos, neg, neu, comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the Logistic Regression classifier\n",
    "logreg_sa_va_1 = LogisticRegression(max_iter=1000)\n",
    "logreg_sa_va_1.fit(train_X_sa_va_1,train_y_sa_va_1)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_logreg_sa_va_1 = logreg_sa_va_1.predict(test_X_sa_va_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_accuracy_sa_va_1 = accuracy_score(y_pred_logreg_sa_va_1, test_y_sa_va_1)\n",
    "print(\"Logistic Regression Accuracy Score -> \", logreg_accuracy_sa_va_1 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aaa658",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y_sa_va_1, y_pred_logreg_sa_va_1, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab37226",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(test_y_sa_va_1, y_pred_logreg_sa_va_1)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cf536",
   "metadata": {},
   "source": [
    "#### VADER1  (comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c86493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the Logistic Regression classifier\n",
    "logreg_sa_va_2 = LogisticRegression(max_iter=200)\n",
    "logreg_sa_va_2.fit(train_X_sa_va_2,train_y_sa_va_2)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_logreg_sa_va_2 = logreg_sa_va_2.predict(test_X_sa_va_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef47c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_accuracy_sa_va_2 = accuracy_score(y_pred_logreg_sa_va_2, test_y_sa_va_2)\n",
    "print(\"Logistic Regression Accuracy Score -> \", logreg_accuracy_sa_va_2 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y_sa_va_2, y_pred_logreg_sa_va_2, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb44f3",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768bfdcb",
   "metadata": {},
   "source": [
    "#### NRC FLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5607a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the SVM classifier\n",
    "linearSVC_sa_nrc = LinearSVC()\n",
    "linearSVC_sa_nrc.fit(train_X_sa_nrc,train_y_sa_nrc)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_SVC_sa_nrc = linearSVC_sa_nrc.predict(test_X_sa_nrc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eb1a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_accuracy_sa_nrc = accuracy_score(y_pred_SVC_sa_nrc, test_y_sa_nrc)\n",
    "print(\"SVM Accuracy Score -> \",svc_accuracy_sa_nrc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cebeacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y_sa_nrc, y_pred_SVC_sa_nrc, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e5a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(test_y_sa_nrc, y_pred_SVC_sa_nrc)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc221aef",
   "metadata": {},
   "source": [
    "#### VADER1  (Pos, neg, neu, comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7dc502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the SVM classifier\n",
    "linearSVC_sa_va_1 = LinearSVC()\n",
    "linearSVC_sa_va_1.fit(train_X_sa_va_1,train_y_sa_va_1)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_SVC_sa_va_1 = linearSVC_sa_va_1.predict(test_X_sa_va_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a070e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_accuracy_sa_va_1 = accuracy_score(y_pred_SVC_sa_va_1, test_y_sa_va_1)\n",
    "print(\"SVM Accuracy Score -> \",svc_accuracy_sa_va_1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y_sa_va_1, y_pred_SVC_sa_va_1, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd31a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(test_y_sa_va_1, y_pred_SVC_sa_va_1)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3c1eb",
   "metadata": {},
   "source": [
    "#### VADER1  (comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26505c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the SVM classifier\n",
    "linearSVC_sa_va_2 = LinearSVC()\n",
    "linearSVC_sa_va_2.fit(train_X_sa_va_2,train_y_sa_va_2)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_SVC_sa_va_2 = linearSVC_sa_va_2.predict(test_X_sa_va_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7335f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_accuracy_sa_va_2 = accuracy_score(y_pred_SVC_sa_va_2, test_y_sa_va_2)\n",
    "print(\"SVM Accuracy Score -> \",svc_accuracy_sa_va_2*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62305c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y_sa_va_2, y_pred_SVC_sa_va_2, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908442a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
