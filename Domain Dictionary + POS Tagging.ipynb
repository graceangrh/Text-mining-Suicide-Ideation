{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4876a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00928adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVAHOME'] = 'C:/Program Files/Java/jdk-19/bin'\n",
    "os.environ['STANFORD_PARSER'] = 'C:/stanford-corenlp-4.5.2'\n",
    "os.environ['STANFORD_MODELS'] = 'C:/stanford-corenlp-4.5.2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59f3942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser\n",
    "\n",
    "pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dbe238",
   "metadata": {},
   "source": [
    "### Domain Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75d8c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = pd.read_csv(\"suicidal_indicator.csv\", header=None).T\n",
    "dict2 = pd.read_csv(\"suicidal_ideation.csv\", header=None).T\n",
    "dict3 = pd.read_csv(\"suicidal_behavior.csv\", header=None).T\n",
    "dict4 = pd.read_csv(\"suicidal_attempt.csv\", header=None).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289291e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pessimistic character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suicide of relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Family history of suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suicide of close relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suicide risk assessment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     lexicons\n",
       "0       Pessimistic character\n",
       "1         Suicide of relative\n",
       "2   Family history of suicide\n",
       "3   Suicide of close relative\n",
       "4     Suicide risk assessment"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###combined dictionary into 1\n",
    "domain_dict = pd.concat([dict1, dict2, dict3,dict4], ignore_index=True)\n",
    "domain_dict = domain_dict.rename(columns={0: 'lexicons'})\n",
    "domain_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "400cd5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_text(text):\n",
    "    text = str(text).lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89450e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict['lexicons'] = domain_dict['lexicons'].apply(lower_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3da51321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pessimistic character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suicide of relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family history of suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide of close relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide risk assessment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>went in the freezer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>jumped from bridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>jumped from roof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>bag around head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>belt around neck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2277 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        lexicons\n",
       "0          pessimistic character\n",
       "1            suicide of relative\n",
       "2      family history of suicide\n",
       "3      suicide of close relative\n",
       "4        suicide risk assessment\n",
       "...                          ...\n",
       "2272         went in the freezer\n",
       "2273          jumped from bridge\n",
       "2274            jumped from roof\n",
       "2275             bag around head\n",
       "2276            belt around neck\n",
       "\n",
       "[2277 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96aed1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict = domain_dict.drop(domain_dict.index[1521]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b30c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_preprocess_text(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    sentence = str(sentence)\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    tagged_words = pos_tagger.tag(words)\n",
    "    tagged_words = [(stemmer.stem(word), tag) for word, tag in tagged_words if word not in stop_words]\n",
    "    return tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf6cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict[\"stem\"] = domain_dict[\"lexicons\"].apply(stem_preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121b889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_preprocess_text(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "#     stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = str(sentence)\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    tagged_words = pos_tagger.tag(words)\n",
    "    tagged_words = [(lemmatizer.lemmatize(word), tag) for word, tag in tagged_words if word not in stop_words]\n",
    "    return tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f5464dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict[\"lem\"] = domain_dict[\"lexicons\"].apply(lem_preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fee2012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "      <th>stem</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pessimistic character</td>\n",
       "      <td>[(pessimist, JJ), (charact, NN)]</td>\n",
       "      <td>[(pessimistic, JJ), (character, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suicide of relative</td>\n",
       "      <td>[(suicid, NN), (relat, JJ)]</td>\n",
       "      <td>[(suicide, NN), (relative, JJ)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family history of suicide</td>\n",
       "      <td>[(famili, NN), (histori, NN), (suicid, NN)]</td>\n",
       "      <td>[(family, NN), (history, NN), (suicide, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide of close relative</td>\n",
       "      <td>[(suicid, NN), (close, JJ), (relat, JJ)]</td>\n",
       "      <td>[(suicide, NN), (close, JJ), (relative, JJ)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide risk assessment</td>\n",
       "      <td>[(suicid, NN), (risk, NN), (assess, NN)]</td>\n",
       "      <td>[(suicide, NN), (risk, NN), (assessment, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>went in the freezer</td>\n",
       "      <td>[(went, VBD), (freezer, NN)]</td>\n",
       "      <td>[(went, VBD), (freezer, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>jumped from bridge</td>\n",
       "      <td>[(jump, VBD), (bridg, NN)]</td>\n",
       "      <td>[(jumped, VBD), (bridge, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>jumped from roof</td>\n",
       "      <td>[(jump, VBD), (roof, NN)]</td>\n",
       "      <td>[(jumped, VBD), (roof, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>bag around head</td>\n",
       "      <td>[(bag, NN), (around, IN), (head, NN)]</td>\n",
       "      <td>[(bag, NN), (around, IN), (head, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>belt around neck</td>\n",
       "      <td>[(belt, NN), (around, IN), (neck, NN)]</td>\n",
       "      <td>[(belt, NN), (around, IN), (neck, NN)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2276 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        lexicons                                         stem  \\\n",
       "0          pessimistic character             [(pessimist, JJ), (charact, NN)]   \n",
       "1            suicide of relative                  [(suicid, NN), (relat, JJ)]   \n",
       "2      family history of suicide  [(famili, NN), (histori, NN), (suicid, NN)]   \n",
       "3      suicide of close relative     [(suicid, NN), (close, JJ), (relat, JJ)]   \n",
       "4        suicide risk assessment     [(suicid, NN), (risk, NN), (assess, NN)]   \n",
       "...                          ...                                          ...   \n",
       "2271         went in the freezer                 [(went, VBD), (freezer, NN)]   \n",
       "2272          jumped from bridge                   [(jump, VBD), (bridg, NN)]   \n",
       "2273            jumped from roof                    [(jump, VBD), (roof, NN)]   \n",
       "2274             bag around head        [(bag, NN), (around, IN), (head, NN)]   \n",
       "2275            belt around neck       [(belt, NN), (around, IN), (neck, NN)]   \n",
       "\n",
       "                                                lem  \n",
       "0              [(pessimistic, JJ), (character, NN)]  \n",
       "1                   [(suicide, NN), (relative, JJ)]  \n",
       "2      [(family, NN), (history, NN), (suicide, NN)]  \n",
       "3      [(suicide, NN), (close, JJ), (relative, JJ)]  \n",
       "4     [(suicide, NN), (risk, NN), (assessment, NN)]  \n",
       "...                                             ...  \n",
       "2271                   [(went, VBD), (freezer, NN)]  \n",
       "2272                  [(jumped, VBD), (bridge, NN)]  \n",
       "2273                    [(jumped, VBD), (roof, NN)]  \n",
       "2274          [(bag, NN), (around, IN), (head, NN)]  \n",
       "2275         [(belt, NN), (around, IN), (neck, NN)]  \n",
       "\n",
       "[2276 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc4f7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_list_to_string(nested_list):\n",
    "    return ' '.join(['_'.join(tup) for tup in nested_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "682e6cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "      <th>stem</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pessimistic character</td>\n",
       "      <td>pessimist_JJ charact_NN</td>\n",
       "      <td>pessimistic_JJ character_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suicide of relative</td>\n",
       "      <td>suicid_NN relat_JJ</td>\n",
       "      <td>suicide_NN relative_JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family history of suicide</td>\n",
       "      <td>famili_NN histori_NN suicid_NN</td>\n",
       "      <td>family_NN history_NN suicide_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide of close relative</td>\n",
       "      <td>suicid_NN close_JJ relat_JJ</td>\n",
       "      <td>suicide_NN close_JJ relative_JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide risk assessment</td>\n",
       "      <td>suicid_NN risk_NN assess_NN</td>\n",
       "      <td>suicide_NN risk_NN assessment_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>went in the freezer</td>\n",
       "      <td>went_VBD freezer_NN</td>\n",
       "      <td>went_VBD freezer_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>jumped from bridge</td>\n",
       "      <td>jump_VBD bridg_NN</td>\n",
       "      <td>jumped_VBD bridge_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>jumped from roof</td>\n",
       "      <td>jump_VBD roof_NN</td>\n",
       "      <td>jumped_VBD roof_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>bag around head</td>\n",
       "      <td>bag_NN around_IN head_NN</td>\n",
       "      <td>bag_NN around_IN head_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>belt around neck</td>\n",
       "      <td>belt_NN around_IN neck_NN</td>\n",
       "      <td>belt_NN around_IN neck_NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2276 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        lexicons                            stem  \\\n",
       "0          pessimistic character         pessimist_JJ charact_NN   \n",
       "1            suicide of relative              suicid_NN relat_JJ   \n",
       "2      family history of suicide  famili_NN histori_NN suicid_NN   \n",
       "3      suicide of close relative     suicid_NN close_JJ relat_JJ   \n",
       "4        suicide risk assessment     suicid_NN risk_NN assess_NN   \n",
       "...                          ...                             ...   \n",
       "2271         went in the freezer             went_VBD freezer_NN   \n",
       "2272          jumped from bridge               jump_VBD bridg_NN   \n",
       "2273            jumped from roof                jump_VBD roof_NN   \n",
       "2274             bag around head        bag_NN around_IN head_NN   \n",
       "2275            belt around neck       belt_NN around_IN neck_NN   \n",
       "\n",
       "                                   lem  \n",
       "0          pessimistic_JJ character_NN  \n",
       "1               suicide_NN relative_JJ  \n",
       "2      family_NN history_NN suicide_NN  \n",
       "3      suicide_NN close_JJ relative_JJ  \n",
       "4     suicide_NN risk_NN assessment_NN  \n",
       "...                                ...  \n",
       "2271               went_VBD freezer_NN  \n",
       "2272              jumped_VBD bridge_NN  \n",
       "2273                jumped_VBD roof_NN  \n",
       "2274          bag_NN around_IN head_NN  \n",
       "2275         belt_NN around_IN neck_NN  \n",
       "\n",
       "[2276 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dict[\"stem\"] = domain_dict[\"stem\"].apply(nested_list_to_string)\n",
    "domain_dict[\"lem\"] = domain_dict[\"lem\"].apply(nested_list_to_string)\n",
    "domain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dec94099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2276"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get list of domain dictionary that are stemmed\n",
    "list_of_stem = []\n",
    "for i in range (len(domain_dict[\"stem\"])):\n",
    "    list_of_stem.append(domain_dict[\"stem\"].iloc[i])\n",
    "len(list_of_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38ee2fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2276"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get list of domain dictionary that are lemmatized\n",
    "list_of_lem = []\n",
    "for j in range (len(domain_dict[\"lem\"])):\n",
    "    list_of_lem.append(domain_dict[\"lem\"].iloc[j])\n",
    "len(list_of_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d97387",
   "metadata": {},
   "source": [
    "### POS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6bb402c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>stemmed_processed_text</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suiciderecently i left my ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threaten', 'suiciderec', 'left...</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suiciderecently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'get', 'affect', 'compliment', 'come...</td>\n",
       "      <td>['weird', 'get', 'affected', 'compliment', 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally      is almost over    so i can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['final', 'almost', 'never', 'hear', 'bad', 'y...</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need helpjust help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'helpjust', 'help', 'cri', 'hard']</td>\n",
       "      <td>['need', 'helpjust', 'help', 'cry', 'hard']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m so losthello  my name is adam      and i v...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['losthello', 'name', 'adam', 'struggl', 'year...</td>\n",
       "      <td>['losthello', 'name', 'adam', 'struggling', 'y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class  \\\n",
       "0  ex wife threatening suiciderecently i left my ...      suicide   \n",
       "1  am i weird i do not get affected by compliment...  non-suicide   \n",
       "2  finally      is almost over    so i can never ...  non-suicide   \n",
       "3        i need helpjust help me i am crying so hard      suicide   \n",
       "4  i m so losthello  my name is adam      and i v...      suicide   \n",
       "\n",
       "                              stemmed_processed_text  \\\n",
       "0  ['ex', 'wife', 'threaten', 'suiciderec', 'left...   \n",
       "1  ['weird', 'get', 'affect', 'compliment', 'come...   \n",
       "2  ['final', 'almost', 'never', 'hear', 'bad', 'y...   \n",
       "3        ['need', 'helpjust', 'help', 'cri', 'hard']   \n",
       "4  ['losthello', 'name', 'adam', 'struggl', 'year...   \n",
       "\n",
       "                           lemmatized_processed_text  \n",
       "0  ['ex', 'wife', 'threatening', 'suiciderecently...  \n",
       "1  ['weird', 'get', 'affected', 'compliment', 'co...  \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...  \n",
       "3        ['need', 'helpjust', 'help', 'cry', 'hard']  \n",
       "4  ['losthello', 'name', 'adam', 'struggling', 'y...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"preprocessed_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef373a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 232074 entries, 0 to 232073\n",
      "Data columns (total 4 columns):\n",
      " #   Column                     Non-Null Count   Dtype \n",
      "---  ------                     --------------   ----- \n",
      " 0   text                       232017 non-null  object\n",
      " 1   class                      232074 non-null  object\n",
      " 2   stemmed_processed_text     232074 non-null  object\n",
      " 3   lemmatized_processed_text  232074 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 7.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6b2fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop stemmed_processed_text column\n",
    "data.drop(columns = [\"stemmed_processed_text\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cf3134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean and tokenize lemmatized_processed_text as it is intepreted as an entire string \n",
    "def clean_and_tokenize(text):\n",
    "    # Remove the brackets and commas using a regular expression\n",
    "    cleaned_text = re.sub(r\"[\\[\\],']\", \"\", text)\n",
    "    # Tokenize the string using word_tokenize\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f51c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'lemmatized_processed_text' column of the DataFrame\n",
    "data['tokens'] = data['lemmatized_processed_text'].apply(clean_and_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fb87977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add POS tags to a list of tokens\n",
    "def add_pos_tags(tokens):\n",
    "    tagged_tokens = []\n",
    "    for token in tokens:\n",
    "        pos_tag = pos_tagger.tag([token])[0][1]\n",
    "        tagged_token = f\"{token}_{pos_tag}\"\n",
    "        tagged_tokens.append(tagged_token)\n",
    "    return tagged_tokens\n",
    "\n",
    "# Apply the function to the 'tokens' column of the DataFrame\n",
    "data['tagged_tokens'] = data['tokens'].map(add_pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3041afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suiciderecently i left my ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suiciderecently...</td>\n",
       "      <td>[ex, wife, threatening, suiciderecently, left,...</td>\n",
       "      <td>[ex_NN, wife_NN, threatening_VBG, suiciderecen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'get', 'affected', 'compliment', 'co...</td>\n",
       "      <td>[weird, get, affected, compliment, coming, som...</td>\n",
       "      <td>[weird_JJ, get_VB, affected_VBN, compliment_NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally      is almost over    so i can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>[finally, almost, never, hear, bad, year, ever...</td>\n",
       "      <td>[finally_RB, almost_RB, never_RB, hear_VB, bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need helpjust help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'helpjust', 'help', 'cry', 'hard']</td>\n",
       "      <td>[need, helpjust, help, cry, hard]</td>\n",
       "      <td>[need_NN, helpjust_RB, help_NN, cry_NN, hard_RB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m so losthello  my name is adam      and i v...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['losthello', 'name', 'adam', 'struggling', 'y...</td>\n",
       "      <td>[losthello, name, adam, struggling, year, afra...</td>\n",
       "      <td>[losthello_NN, name_NN, adam_NN, struggling_VB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class  \\\n",
       "0  ex wife threatening suiciderecently i left my ...      suicide   \n",
       "1  am i weird i do not get affected by compliment...  non-suicide   \n",
       "2  finally      is almost over    so i can never ...  non-suicide   \n",
       "3        i need helpjust help me i am crying so hard      suicide   \n",
       "4  i m so losthello  my name is adam      and i v...      suicide   \n",
       "\n",
       "                           lemmatized_processed_text  \\\n",
       "0  ['ex', 'wife', 'threatening', 'suiciderecently...   \n",
       "1  ['weird', 'get', 'affected', 'compliment', 'co...   \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3        ['need', 'helpjust', 'help', 'cry', 'hard']   \n",
       "4  ['losthello', 'name', 'adam', 'struggling', 'y...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [ex, wife, threatening, suiciderecently, left,...   \n",
       "1  [weird, get, affected, compliment, coming, som...   \n",
       "2  [finally, almost, never, hear, bad, year, ever...   \n",
       "3                  [need, helpjust, help, cry, hard]   \n",
       "4  [losthello, name, adam, struggling, year, afra...   \n",
       "\n",
       "                                       tagged_tokens  \n",
       "0  [ex_NN, wife_NN, threatening_VBG, suiciderecen...  \n",
       "1  [weird_JJ, get_VB, affected_VBN, compliment_NN...  \n",
       "2  [finally_RB, almost_RB, never_RB, hear_VB, bad...  \n",
       "3   [need_NN, helpjust_RB, help_NN, cry_NN, hard_RB]  \n",
       "4  [losthello_NN, name_NN, adam_NN, struggling_VB...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbc2b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('pos_tagged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e50180",
   "metadata": {},
   "source": [
    "### combining domain dictionary + POS and creating TF-IDF model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b94b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"pos_tagged.csv\")\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ae981be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suiciderecently i left my ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suiciderecently...</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suiciderecently...</td>\n",
       "      <td>['ex_NN', 'wife_NN', 'threatening_VBG', 'suici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'get', 'affected', 'compliment', 'co...</td>\n",
       "      <td>['weird', 'get', 'affected', 'compliment', 'co...</td>\n",
       "      <td>['weird_JJ', 'get_VB', 'affected_VBN', 'compli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally      is almost over    so i can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>['finally_RB', 'almost_RB', 'never_RB', 'hear_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need helpjust help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'helpjust', 'help', 'cry', 'hard']</td>\n",
       "      <td>['need', 'helpjust', 'help', 'cry', 'hard']</td>\n",
       "      <td>['need_NN', 'helpjust_RB', 'help_NN', 'cry_NN'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m so losthello  my name is adam      and i v...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['losthello', 'name', 'adam', 'struggling', 'y...</td>\n",
       "      <td>['losthello', 'name', 'adam', 'struggling', 'y...</td>\n",
       "      <td>['losthello_NN', 'name_NN', 'adam_NN', 'strugg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232069</th>\n",
       "      <td>if you do not like rock then your not going to...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['like', 'rock', 'going', 'get', 'anything', '...</td>\n",
       "      <td>['like', 'rock', 'going', 'get', 'anything', '...</td>\n",
       "      <td>['like_UH', 'rock_NN', 'going_VBG', 'get_VB', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232070</th>\n",
       "      <td>you how you can tell i have so many friends an...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['tell', 'many', 'friend', 'lonely', 'everythi...</td>\n",
       "      <td>['tell', 'many', 'friend', 'lonely', 'everythi...</td>\n",
       "      <td>['tell_VB', 'many_JJ', 'friend_NN', 'lonely_JJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232071</th>\n",
       "      <td>pee probably tastes like salty tea     can som...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['pee', 'probably', 'taste', 'like', 'salty', ...</td>\n",
       "      <td>['pee', 'probably', 'taste', 'like', 'salty', ...</td>\n",
       "      <td>['pee_VB', 'probably_RB', 'taste_NN', 'like_UH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232072</th>\n",
       "      <td>the usual stuff you find herei'm not posting t...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['usual', 'stuff', 'find', 'herei', \"'m\", 'pos...</td>\n",
       "      <td>['usual', 'stuff', 'find', 'herei', '``', 'm',...</td>\n",
       "      <td>['usual_JJ', 'stuff_NN', 'find_VB', 'herei_RB'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232073</th>\n",
       "      <td>i still have not beaten the first boss in holl...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['still', 'beaten', 'first', 'bos', 'hollow', ...</td>\n",
       "      <td>['still', 'beaten', 'first', 'bos', 'hollow', ...</td>\n",
       "      <td>['still_RB', 'beaten_VBN', 'first_RB', 'bos_NN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232017 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text        class  \\\n",
       "0       ex wife threatening suiciderecently i left my ...      suicide   \n",
       "1       am i weird i do not get affected by compliment...  non-suicide   \n",
       "2       finally      is almost over    so i can never ...  non-suicide   \n",
       "3             i need helpjust help me i am crying so hard      suicide   \n",
       "4       i m so losthello  my name is adam      and i v...      suicide   \n",
       "...                                                   ...          ...   \n",
       "232069  if you do not like rock then your not going to...  non-suicide   \n",
       "232070  you how you can tell i have so many friends an...  non-suicide   \n",
       "232071  pee probably tastes like salty tea     can som...  non-suicide   \n",
       "232072  the usual stuff you find herei'm not posting t...      suicide   \n",
       "232073  i still have not beaten the first boss in holl...  non-suicide   \n",
       "\n",
       "                                lemmatized_processed_text  \\\n",
       "0       ['ex', 'wife', 'threatening', 'suiciderecently...   \n",
       "1       ['weird', 'get', 'affected', 'compliment', 'co...   \n",
       "2       ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3             ['need', 'helpjust', 'help', 'cry', 'hard']   \n",
       "4       ['losthello', 'name', 'adam', 'struggling', 'y...   \n",
       "...                                                   ...   \n",
       "232069  ['like', 'rock', 'going', 'get', 'anything', '...   \n",
       "232070  ['tell', 'many', 'friend', 'lonely', 'everythi...   \n",
       "232071  ['pee', 'probably', 'taste', 'like', 'salty', ...   \n",
       "232072  ['usual', 'stuff', 'find', 'herei', \"'m\", 'pos...   \n",
       "232073  ['still', 'beaten', 'first', 'bos', 'hollow', ...   \n",
       "\n",
       "                                                   tokens  \\\n",
       "0       ['ex', 'wife', 'threatening', 'suiciderecently...   \n",
       "1       ['weird', 'get', 'affected', 'compliment', 'co...   \n",
       "2       ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3             ['need', 'helpjust', 'help', 'cry', 'hard']   \n",
       "4       ['losthello', 'name', 'adam', 'struggling', 'y...   \n",
       "...                                                   ...   \n",
       "232069  ['like', 'rock', 'going', 'get', 'anything', '...   \n",
       "232070  ['tell', 'many', 'friend', 'lonely', 'everythi...   \n",
       "232071  ['pee', 'probably', 'taste', 'like', 'salty', ...   \n",
       "232072  ['usual', 'stuff', 'find', 'herei', '``', 'm',...   \n",
       "232073  ['still', 'beaten', 'first', 'bos', 'hollow', ...   \n",
       "\n",
       "                                            tagged_tokens  \n",
       "0       ['ex_NN', 'wife_NN', 'threatening_VBG', 'suici...  \n",
       "1       ['weird_JJ', 'get_VB', 'affected_VBN', 'compli...  \n",
       "2       ['finally_RB', 'almost_RB', 'never_RB', 'hear_...  \n",
       "3       ['need_NN', 'helpjust_RB', 'help_NN', 'cry_NN'...  \n",
       "4       ['losthello_NN', 'name_NN', 'adam_NN', 'strugg...  \n",
       "...                                                   ...  \n",
       "232069  ['like_UH', 'rock_NN', 'going_VBG', 'get_VB', ...  \n",
       "232070  ['tell_VB', 'many_JJ', 'friend_NN', 'lonely_JJ...  \n",
       "232071  ['pee_VB', 'probably_RB', 'taste_NN', 'like_UH...  \n",
       "232072  ['usual_JJ', 'stuff_NN', 'find_VB', 'herei_RB'...  \n",
       "232073  ['still_RB', 'beaten_VBN', 'first_RB', 'bos_NN...  \n",
       "\n",
       "[232017 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcb962ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "data['tagged_tokens'] = data['tagged_tokens'].apply(lambda x: [str(i) for i in ast.literal_eval(x)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d240a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>suicide</td>\n",
       "      <td>[ex_NN, wife_NN, threatening_VBG, suiciderecen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>[weird_JJ, get_VB, affected_VBN, compliment_NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>[finally_RB, almost_RB, never_RB, hear_VB, bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide</td>\n",
       "      <td>[need_NN, helpjust_RB, help_NN, cry_NN, hard_RB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide</td>\n",
       "      <td>[losthello_NN, name_NN, adam_NN, struggling_VB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232069</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>[like_UH, rock_NN, going_VBG, get_VB, anything...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232070</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>[tell_VB, many_JJ, friend_NN, lonely_JJ, every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232071</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>[pee_VB, probably_RB, taste_NN, like_UH, salty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232072</th>\n",
       "      <td>suicide</td>\n",
       "      <td>[usual_JJ, stuff_NN, find_VB, herei_RB, ``_``,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232073</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>[still_RB, beaten_VBN, first_RB, bos_NNS, holl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232017 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              class                                      tagged_tokens\n",
       "0           suicide  [ex_NN, wife_NN, threatening_VBG, suiciderecen...\n",
       "1       non-suicide  [weird_JJ, get_VB, affected_VBN, compliment_NN...\n",
       "2       non-suicide  [finally_RB, almost_RB, never_RB, hear_VB, bad...\n",
       "3           suicide   [need_NN, helpjust_RB, help_NN, cry_NN, hard_RB]\n",
       "4           suicide  [losthello_NN, name_NN, adam_NN, struggling_VB...\n",
       "...             ...                                                ...\n",
       "232069  non-suicide  [like_UH, rock_NN, going_VBG, get_VB, anything...\n",
       "232070  non-suicide  [tell_VB, many_JJ, friend_NN, lonely_JJ, every...\n",
       "232071  non-suicide  [pee_VB, probably_RB, taste_NN, like_UH, salty...\n",
       "232072      suicide  [usual_JJ, stuff_NN, find_VB, herei_RB, ``_``,...\n",
       "232073  non-suicide  [still_RB, beaten_VBN, first_RB, bos_NNS, holl...\n",
       "\n",
       "[232017 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns = [\"text\",\"lemmatized_processed_text\",\"tokens\"], inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc90503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "011f536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(data['tagged_tokens'],data['class'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebc6d5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162411"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get all words from the document with pos_tag\n",
    "\n",
    "tagged_tokens_list = []\n",
    "for i in range (len(train_X)):\n",
    "    sen = ' '.join(train_X.iloc[i])\n",
    "    tagged_tokens_list.append(sen)\n",
    "len(tagged_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d633dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162411.9"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)*0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c58669b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164687"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combining the document and dictionary\n",
    "tagged_tokens_list.extend(list_of_lem)\n",
    "len(tagged_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0595cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_vector = tfidf.fit(tagged_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62b5e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "train_y = Encoder.fit_transform(train_y)\n",
    "test_y = Encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86e0014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_Tfidf = tfidf_vector.transform(train_X.apply(lambda x: ' '.join(x)))\n",
    "test_X_Tfidf = tfidf_vector.transform(test_X.apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d82aac65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  88.45358158779416\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(train_X_Tfidf,train_y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, test_y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c060427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score ->  93.43734735511306\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the Logistic Regression classifier\n",
    "logreg = LogisticRegression(max_iter=200)\n",
    "logreg.fit(train_X_Tfidf,train_y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_logreg = logreg.predict(test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Logistic Regression Accuracy Score -> \",accuracy_score(predictions_logreg, test_y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5823a52c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b21f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
