{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af4876a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00928adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVAHOME'] = 'C:/Program Files/Java/jdk-17.0.1/bin'\n",
    "os.environ['STANFORD_PARSER'] = 'C:/stanford-corenlp-4.5.3'\n",
    "os.environ['STANFORD_MODELS'] = 'C:/stanford-corenlp-4.5.3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59f3942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser\n",
    "\n",
    "pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dbe238",
   "metadata": {},
   "source": [
    "### Domain Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e75d8c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = pd.read_csv(\"suicidal_indicator.csv\", header=None).T\n",
    "dict2 = pd.read_csv(\"suicidal_ideation.csv\", header=None).T\n",
    "dict3 = pd.read_csv(\"suicidal_behavior.csv\", header=None).T\n",
    "dict4 = pd.read_csv(\"suicidal_attempt.csv\", header=None).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "289291e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pessimistic character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suicide of relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Family history of suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suicide of close relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suicide risk assessment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     lexicons\n",
       "0       Pessimistic character\n",
       "1         Suicide of relative\n",
       "2   Family history of suicide\n",
       "3   Suicide of close relative\n",
       "4     Suicide risk assessment"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###combined dictionary into 1\n",
    "domain_dict = pd.concat([dict1, dict2, dict3,dict4], ignore_index=True)\n",
    "domain_dict = domain_dict.rename(columns={0: 'lexicons'})\n",
    "domain_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "400cd5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_text(text):\n",
    "    text = str(text).lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c89450e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict['lexicons'] = domain_dict['lexicons'].apply(lower_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3da51321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pessimistic character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suicide of relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family history of suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide of close relative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide risk assessment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>went in the freezer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>jumped from bridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>jumped from roof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>bag around head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>belt around neck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2277 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        lexicons\n",
       "0          pessimistic character\n",
       "1            suicide of relative\n",
       "2      family history of suicide\n",
       "3      suicide of close relative\n",
       "4        suicide risk assessment\n",
       "...                          ...\n",
       "2272         went in the freezer\n",
       "2273          jumped from bridge\n",
       "2274            jumped from roof\n",
       "2275             bag around head\n",
       "2276            belt around neck\n",
       "\n",
       "[2277 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96aed1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict = domain_dict.drop(domain_dict.index[1521]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b30c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_preprocess_text(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    sentence = str(sentence)\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    tagged_words = pos_tagger.tag(words)\n",
    "    tagged_words = [(stemmer.stem(word), tag) for word, tag in tagged_words if word not in stop_words]\n",
    "    return tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daf6cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict[\"stem\"] = domain_dict[\"lexicons\"].apply(stem_preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "121b889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_preprocess_text(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "#     stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = str(sentence)\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    tagged_words = pos_tagger.tag(words)\n",
    "    tagged_words = [(lemmatizer.lemmatize(word), tag) for word, tag in tagged_words if word not in stop_words]\n",
    "    return tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f5464dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dict[\"lem\"] = domain_dict[\"lexicons\"].apply(lem_preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fee2012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "      <th>stem</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pessimistic character</td>\n",
       "      <td>[(pessimist, JJ), (charact, NN)]</td>\n",
       "      <td>[(pessimistic, JJ), (character, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suicide of relative</td>\n",
       "      <td>[(suicid, NN), (relat, JJ)]</td>\n",
       "      <td>[(suicide, NN), (relative, JJ)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family history of suicide</td>\n",
       "      <td>[(famili, NN), (histori, NN), (suicid, NN)]</td>\n",
       "      <td>[(family, NN), (history, NN), (suicide, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide of close relative</td>\n",
       "      <td>[(suicid, NN), (close, JJ), (relat, JJ)]</td>\n",
       "      <td>[(suicide, NN), (close, JJ), (relative, JJ)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide risk assessment</td>\n",
       "      <td>[(suicid, NN), (risk, NN), (assess, NN)]</td>\n",
       "      <td>[(suicide, NN), (risk, NN), (assessment, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>went in the freezer</td>\n",
       "      <td>[(went, VBD), (freezer, NN)]</td>\n",
       "      <td>[(went, VBD), (freezer, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>jumped from bridge</td>\n",
       "      <td>[(jump, VBD), (bridg, NN)]</td>\n",
       "      <td>[(jumped, VBD), (bridge, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>jumped from roof</td>\n",
       "      <td>[(jump, VBD), (roof, NN)]</td>\n",
       "      <td>[(jumped, VBD), (roof, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>bag around head</td>\n",
       "      <td>[(bag, NN), (around, IN), (head, NN)]</td>\n",
       "      <td>[(bag, NN), (around, IN), (head, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>belt around neck</td>\n",
       "      <td>[(belt, NN), (around, IN), (neck, NN)]</td>\n",
       "      <td>[(belt, NN), (around, IN), (neck, NN)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2276 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        lexicons                                         stem  \\\n",
       "0          pessimistic character             [(pessimist, JJ), (charact, NN)]   \n",
       "1            suicide of relative                  [(suicid, NN), (relat, JJ)]   \n",
       "2      family history of suicide  [(famili, NN), (histori, NN), (suicid, NN)]   \n",
       "3      suicide of close relative     [(suicid, NN), (close, JJ), (relat, JJ)]   \n",
       "4        suicide risk assessment     [(suicid, NN), (risk, NN), (assess, NN)]   \n",
       "...                          ...                                          ...   \n",
       "2271         went in the freezer                 [(went, VBD), (freezer, NN)]   \n",
       "2272          jumped from bridge                   [(jump, VBD), (bridg, NN)]   \n",
       "2273            jumped from roof                    [(jump, VBD), (roof, NN)]   \n",
       "2274             bag around head        [(bag, NN), (around, IN), (head, NN)]   \n",
       "2275            belt around neck       [(belt, NN), (around, IN), (neck, NN)]   \n",
       "\n",
       "                                                lem  \n",
       "0              [(pessimistic, JJ), (character, NN)]  \n",
       "1                   [(suicide, NN), (relative, JJ)]  \n",
       "2      [(family, NN), (history, NN), (suicide, NN)]  \n",
       "3      [(suicide, NN), (close, JJ), (relative, JJ)]  \n",
       "4     [(suicide, NN), (risk, NN), (assessment, NN)]  \n",
       "...                                             ...  \n",
       "2271                   [(went, VBD), (freezer, NN)]  \n",
       "2272                  [(jumped, VBD), (bridge, NN)]  \n",
       "2273                    [(jumped, VBD), (roof, NN)]  \n",
       "2274          [(bag, NN), (around, IN), (head, NN)]  \n",
       "2275         [(belt, NN), (around, IN), (neck, NN)]  \n",
       "\n",
       "[2276 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc4f7172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_list_to_string(nested_list):\n",
    "    return ' '.join(['_'.join(tup) for tup in nested_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "682e6cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexicons</th>\n",
       "      <th>stem</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pessimistic character</td>\n",
       "      <td>pessimist_JJ charact_NN</td>\n",
       "      <td>pessimistic_JJ character_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>suicide of relative</td>\n",
       "      <td>suicid_NN relat_JJ</td>\n",
       "      <td>suicide_NN relative_JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family history of suicide</td>\n",
       "      <td>famili_NN histori_NN suicid_NN</td>\n",
       "      <td>family_NN history_NN suicide_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide of close relative</td>\n",
       "      <td>suicid_NN close_JJ relat_JJ</td>\n",
       "      <td>suicide_NN close_JJ relative_JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide risk assessment</td>\n",
       "      <td>suicid_NN risk_NN assess_NN</td>\n",
       "      <td>suicide_NN risk_NN assessment_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>went in the freezer</td>\n",
       "      <td>went_VBD freezer_NN</td>\n",
       "      <td>went_VBD freezer_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>jumped from bridge</td>\n",
       "      <td>jump_VBD bridg_NN</td>\n",
       "      <td>jumped_VBD bridge_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>jumped from roof</td>\n",
       "      <td>jump_VBD roof_NN</td>\n",
       "      <td>jumped_VBD roof_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>bag around head</td>\n",
       "      <td>bag_NN around_IN head_NN</td>\n",
       "      <td>bag_NN around_IN head_NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>belt around neck</td>\n",
       "      <td>belt_NN around_IN neck_NN</td>\n",
       "      <td>belt_NN around_IN neck_NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2276 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        lexicons                            stem  \\\n",
       "0          pessimistic character         pessimist_JJ charact_NN   \n",
       "1            suicide of relative              suicid_NN relat_JJ   \n",
       "2      family history of suicide  famili_NN histori_NN suicid_NN   \n",
       "3      suicide of close relative     suicid_NN close_JJ relat_JJ   \n",
       "4        suicide risk assessment     suicid_NN risk_NN assess_NN   \n",
       "...                          ...                             ...   \n",
       "2271         went in the freezer             went_VBD freezer_NN   \n",
       "2272          jumped from bridge               jump_VBD bridg_NN   \n",
       "2273            jumped from roof                jump_VBD roof_NN   \n",
       "2274             bag around head        bag_NN around_IN head_NN   \n",
       "2275            belt around neck       belt_NN around_IN neck_NN   \n",
       "\n",
       "                                   lem  \n",
       "0          pessimistic_JJ character_NN  \n",
       "1               suicide_NN relative_JJ  \n",
       "2      family_NN history_NN suicide_NN  \n",
       "3      suicide_NN close_JJ relative_JJ  \n",
       "4     suicide_NN risk_NN assessment_NN  \n",
       "...                                ...  \n",
       "2271               went_VBD freezer_NN  \n",
       "2272              jumped_VBD bridge_NN  \n",
       "2273                jumped_VBD roof_NN  \n",
       "2274          bag_NN around_IN head_NN  \n",
       "2275         belt_NN around_IN neck_NN  \n",
       "\n",
       "[2276 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dict[\"stem\"] = domain_dict[\"stem\"].apply(nested_list_to_string)\n",
    "domain_dict[\"lem\"] = domain_dict[\"lem\"].apply(nested_list_to_string)\n",
    "domain_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dec94099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2276"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get list of domain dictionary that are stemmed\n",
    "list_of_stem = []\n",
    "for i in range (len(domain_dict[\"stem\"])):\n",
    "    list_of_stem.append(domain_dict[\"stem\"].iloc[i])\n",
    "len(list_of_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38ee2fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2276"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get list of domain dictionary that are lemmatized\n",
    "list_of_lem = []\n",
    "for j in range (len(domain_dict[\"lem\"])):\n",
    "    list_of_lem.append(domain_dict[\"lem\"].iloc[j])\n",
    "len(list_of_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d97387",
   "metadata": {},
   "source": [
    "### POS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"preprocessed_data_new.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef373a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b2fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop stemmed_processed_text column\n",
    "data.drop(columns = [\"stemmed_processed_text\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf3134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean and tokenize lemmatized_processed_text as it is intepreted as an entire string \n",
    "def clean_and_tokenize(text):\n",
    "    # Remove the brackets and commas using a regular expression\n",
    "    cleaned_text = re.sub(r\"[\\[\\],']\", \"\", text)\n",
    "    # Tokenize the string using word_tokenize\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'lemmatized_processed_text' column of the DataFrame\n",
    "data['tokens'] = data['lemmatized_processed_text'].apply(clean_and_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fb87977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add POS tags to a list of tokens\n",
    "def add_pos_tags(tokens):\n",
    "    tagged_tokens = []\n",
    "    for token in tokens:\n",
    "        pos_tag = pos_tagger.tag([token])[0][1]\n",
    "        tagged_token = f\"{token}_{pos_tag}\"\n",
    "        tagged_tokens.append(tagged_token)\n",
    "    return tagged_tokens\n",
    "\n",
    "# Apply the function to the 'tokens' column of the DataFrame\n",
    "data['tagged_tokens'] = data['tokens'].map(add_pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3041afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently i left my...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suicide', 'rece...</td>\n",
       "      <td>[ex, wife, threatening, suicide, recently, lef...</td>\n",
       "      <td>[ex_NN, wife_NN, threatening_VBG, suicide_NN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'get', 'affected', 'compliment', 'co...</td>\n",
       "      <td>[weird, get, affected, compliment, coming, som...</td>\n",
       "      <td>[weird_JJ, get_VB, affected_VBN, compliment_NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally is almost over so i can never hear has...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>[finally, almost, never, hear, bad, year, ever...</td>\n",
       "      <td>[finally_RB, almost_RB, never_RB, hear_VB, bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need help just help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'help', 'help', 'cry', 'hard']</td>\n",
       "      <td>[need, help, help, cry, hard]</td>\n",
       "      <td>[need_NN, help_NN, help_NN, cry_NN, hard_RB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m so lost hello my name is adam and i ve bee...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggling'...</td>\n",
       "      <td>[lost, hello, name, adam, struggling, year, af...</td>\n",
       "      <td>[lost_VBN, hello_UH, name_NN, adam_NN, struggl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class  \\\n",
       "0  ex wife threatening suicide recently i left my...      suicide   \n",
       "1  am i weird i do not get affected by compliment...  non-suicide   \n",
       "2  finally is almost over so i can never hear has...  non-suicide   \n",
       "3       i need help just help me i am crying so hard      suicide   \n",
       "4  i m so lost hello my name is adam and i ve bee...      suicide   \n",
       "\n",
       "                           lemmatized_processed_text  \\\n",
       "0  ['ex', 'wife', 'threatening', 'suicide', 'rece...   \n",
       "1  ['weird', 'get', 'affected', 'compliment', 'co...   \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3            ['need', 'help', 'help', 'cry', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggling'...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [ex, wife, threatening, suicide, recently, lef...   \n",
       "1  [weird, get, affected, compliment, coming, som...   \n",
       "2  [finally, almost, never, hear, bad, year, ever...   \n",
       "3                      [need, help, help, cry, hard]   \n",
       "4  [lost, hello, name, adam, struggling, year, af...   \n",
       "\n",
       "                                       tagged_tokens  \n",
       "0  [ex_NN, wife_NN, threatening_VBG, suicide_NN, ...  \n",
       "1  [weird_JJ, get_VB, affected_VBN, compliment_NN...  \n",
       "2  [finally_RB, almost_RB, never_RB, hear_VB, bad...  \n",
       "3       [need_NN, help_NN, help_NN, cry_NN, hard_RB]  \n",
       "4  [lost_VBN, hello_UH, name_NN, adam_NN, struggl...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbc2b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('pos_tagged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e50180",
   "metadata": {},
   "source": [
    "### combining domain dictionary + POS and creating TF-IDF model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b94b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"pos_tagged_new.csv\")\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2284363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a copy of data\n",
    "data_text = data[\"text\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ae981be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently i left my...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suicide', 'rece...</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suicide', 'rece...</td>\n",
       "      <td>['ex_NN', 'wife_NN', 'threatening_VBG', 'suici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'get', 'affected', 'compliment', 'co...</td>\n",
       "      <td>['weird', 'get', 'affected', 'compliment', 'co...</td>\n",
       "      <td>['weird_JJ', 'get_VB', 'affected_VBN', 'compli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally is almost over so i can never hear has...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>['finally_RB', 'almost_RB', 'never_RB', 'hear_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need help just help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'help', 'help', 'cry', 'hard']</td>\n",
       "      <td>['need', 'help', 'help', 'cry', 'hard']</td>\n",
       "      <td>['need_NN', 'help_NN', 'help_NN', 'cry_NN', 'h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m so lost hello my name is adam and i ve bee...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggling'...</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggling'...</td>\n",
       "      <td>['lost_VBN', 'hello_UH', 'name_NN', 'adam_NN',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232069</th>\n",
       "      <td>if you do not like rock then your not going to...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['like', 'rock', 'going', 'get', 'anything', '...</td>\n",
       "      <td>['like', 'rock', 'going', 'get', 'anything', '...</td>\n",
       "      <td>['like_UH', 'rock_NN', 'going_VBG', 'get_VB', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232070</th>\n",
       "      <td>you how you can tell i have so many friends an...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['tell', 'many', 'friend', 'lonely', 'everythi...</td>\n",
       "      <td>['tell', 'many', 'friend', 'lonely', 'everythi...</td>\n",
       "      <td>['tell_VB', 'many_JJ', 'friend_NN', 'lonely_JJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232071</th>\n",
       "      <td>pee probably tastes like salty tea can someone...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['pee', 'probably', 'taste', 'like', 'salty', ...</td>\n",
       "      <td>['pee', 'probably', 'taste', 'like', 'salty', ...</td>\n",
       "      <td>['pee_VB', 'probably_RB', 'taste_NN', 'like_UH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232072</th>\n",
       "      <td>the usual stuff you find here i'm not posting ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['usual', 'stuff', 'find', \"'m\", 'posting', 's...</td>\n",
       "      <td>['usual', 'stuff', 'find', '``', 'm', \"''\", 'p...</td>\n",
       "      <td>['usual_JJ', 'stuff_NN', 'find_VB', '``_``', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232073</th>\n",
       "      <td>i still have not beaten the first boss in holl...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['still', 'beaten', 'first', 'bos', 'hollow', ...</td>\n",
       "      <td>['still', 'beaten', 'first', 'bos', 'hollow', ...</td>\n",
       "      <td>['still_RB', 'beaten_VBN', 'first_RB', 'bos_NN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232017 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text        class  \\\n",
       "0       ex wife threatening suicide recently i left my...      suicide   \n",
       "1       am i weird i do not get affected by compliment...  non-suicide   \n",
       "2       finally is almost over so i can never hear has...  non-suicide   \n",
       "3            i need help just help me i am crying so hard      suicide   \n",
       "4       i m so lost hello my name is adam and i ve bee...      suicide   \n",
       "...                                                   ...          ...   \n",
       "232069  if you do not like rock then your not going to...  non-suicide   \n",
       "232070  you how you can tell i have so many friends an...  non-suicide   \n",
       "232071  pee probably tastes like salty tea can someone...  non-suicide   \n",
       "232072  the usual stuff you find here i'm not posting ...      suicide   \n",
       "232073  i still have not beaten the first boss in holl...  non-suicide   \n",
       "\n",
       "                                lemmatized_processed_text  \\\n",
       "0       ['ex', 'wife', 'threatening', 'suicide', 'rece...   \n",
       "1       ['weird', 'get', 'affected', 'compliment', 'co...   \n",
       "2       ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3                 ['need', 'help', 'help', 'cry', 'hard']   \n",
       "4       ['lost', 'hello', 'name', 'adam', 'struggling'...   \n",
       "...                                                   ...   \n",
       "232069  ['like', 'rock', 'going', 'get', 'anything', '...   \n",
       "232070  ['tell', 'many', 'friend', 'lonely', 'everythi...   \n",
       "232071  ['pee', 'probably', 'taste', 'like', 'salty', ...   \n",
       "232072  ['usual', 'stuff', 'find', \"'m\", 'posting', 's...   \n",
       "232073  ['still', 'beaten', 'first', 'bos', 'hollow', ...   \n",
       "\n",
       "                                                   tokens  \\\n",
       "0       ['ex', 'wife', 'threatening', 'suicide', 'rece...   \n",
       "1       ['weird', 'get', 'affected', 'compliment', 'co...   \n",
       "2       ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3                 ['need', 'help', 'help', 'cry', 'hard']   \n",
       "4       ['lost', 'hello', 'name', 'adam', 'struggling'...   \n",
       "...                                                   ...   \n",
       "232069  ['like', 'rock', 'going', 'get', 'anything', '...   \n",
       "232070  ['tell', 'many', 'friend', 'lonely', 'everythi...   \n",
       "232071  ['pee', 'probably', 'taste', 'like', 'salty', ...   \n",
       "232072  ['usual', 'stuff', 'find', '``', 'm', \"''\", 'p...   \n",
       "232073  ['still', 'beaten', 'first', 'bos', 'hollow', ...   \n",
       "\n",
       "                                            tagged_tokens  \n",
       "0       ['ex_NN', 'wife_NN', 'threatening_VBG', 'suici...  \n",
       "1       ['weird_JJ', 'get_VB', 'affected_VBN', 'compli...  \n",
       "2       ['finally_RB', 'almost_RB', 'never_RB', 'hear_...  \n",
       "3       ['need_NN', 'help_NN', 'help_NN', 'cry_NN', 'h...  \n",
       "4       ['lost_VBN', 'hello_UH', 'name_NN', 'adam_NN',...  \n",
       "...                                                   ...  \n",
       "232069  ['like_UH', 'rock_NN', 'going_VBG', 'get_VB', ...  \n",
       "232070  ['tell_VB', 'many_JJ', 'friend_NN', 'lonely_JJ...  \n",
       "232071  ['pee_VB', 'probably_RB', 'taste_NN', 'like_UH...  \n",
       "232072  ['usual_JJ', 'stuff_NN', 'find_VB', '``_``', '...  \n",
       "232073  ['still_RB', 'beaten_VBN', 'first_RB', 'bos_NN...  \n",
       "\n",
       "[232017 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcb962ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "data['tagged_tokens'] = data['tagged_tokens'].apply(lambda x: [str(i) for i in ast.literal_eval(x)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d240a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>suicide</td>\n",
       "      <td>[ex_NN, wife_NN, threatening_VBG, suicide_NN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>[weird_JJ, get_VB, affected_VBN, compliment_NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>[finally_RB, almost_RB, never_RB, hear_VB, bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide</td>\n",
       "      <td>[need_NN, help_NN, help_NN, cry_NN, hard_RB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide</td>\n",
       "      <td>[lost_VBN, hello_UH, name_NN, adam_NN, struggl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232069</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>[like_UH, rock_NN, going_VBG, get_VB, anything...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232070</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>[tell_VB, many_JJ, friend_NN, lonely_JJ, every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232071</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>[pee_VB, probably_RB, taste_NN, like_UH, salty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232072</th>\n",
       "      <td>suicide</td>\n",
       "      <td>[usual_JJ, stuff_NN, find_VB, ``_``, m_NN, ''_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232073</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>[still_RB, beaten_VBN, first_RB, bos_NNS, holl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232017 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              class                                      tagged_tokens\n",
       "0           suicide  [ex_NN, wife_NN, threatening_VBG, suicide_NN, ...\n",
       "1       non-suicide  [weird_JJ, get_VB, affected_VBN, compliment_NN...\n",
       "2       non-suicide  [finally_RB, almost_RB, never_RB, hear_VB, bad...\n",
       "3           suicide       [need_NN, help_NN, help_NN, cry_NN, hard_RB]\n",
       "4           suicide  [lost_VBN, hello_UH, name_NN, adam_NN, struggl...\n",
       "...             ...                                                ...\n",
       "232069  non-suicide  [like_UH, rock_NN, going_VBG, get_VB, anything...\n",
       "232070  non-suicide  [tell_VB, many_JJ, friend_NN, lonely_JJ, every...\n",
       "232071  non-suicide  [pee_VB, probably_RB, taste_NN, like_UH, salty...\n",
       "232072      suicide  [usual_JJ, stuff_NN, find_VB, ``_``, m_NN, ''_...\n",
       "232073  non-suicide  [still_RB, beaten_VBN, first_RB, bos_NNS, holl...\n",
       "\n",
       "[232017 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns = [\"text\",\"lemmatized_processed_text\",\"tokens\"], inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ef07350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>suicide</td>\n",
       "      <td>ex_NN wife_NN threatening_VBG suicide_NN recen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>weird_JJ get_VB affected_VBN compliment_NN com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>finally_RB almost_RB never_RB hear_VB bad_JJ y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide</td>\n",
       "      <td>need_NN help_NN help_NN cry_NN hard_RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide</td>\n",
       "      <td>lost_VBN hello_UH name_NN adam_NN struggling_V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class                                      tagged_tokens\n",
       "0      suicide  ex_NN wife_NN threatening_VBG suicide_NN recen...\n",
       "1  non-suicide  weird_JJ get_VB affected_VBN compliment_NN com...\n",
       "2  non-suicide  finally_RB almost_RB never_RB hear_VB bad_JJ y...\n",
       "3      suicide             need_NN help_NN help_NN cry_NN hard_RB\n",
       "4      suicide  lost_VBN hello_UH name_NN adam_NN struggling_V..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tagged_tokens\"] = data[\"tagged_tokens\"].apply(lambda x: \" \".join(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc90503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "011f536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(data['tagged_tokens'],data['class'],test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "080a436c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13686     losing_VBG grip_NN parent_NN gave_VBD old_JJ n...\n",
       "60022     hey_UH yy_NNP mean_VB got_VBD sent_VBN right_U...\n",
       "179581    people_NNS hate_NN much_RB commit_VB die_VB wo...\n",
       "117440    failing_VBG school_NN might_MD right_UH place_...\n",
       "90275     like_UH parent_NN ``_`` ''_'' door_NN locked_V...\n",
       "                                ...                        \n",
       "119913    get_VB random_JJ fact_NN day_NN decided_VBN ma...\n",
       "103725                                         situation_NN\n",
       "131967    know_VB else_RB turn_NN recently_RB got_VBD di...\n",
       "146907    would_MD like_UH share_NN something_NN watch_N...\n",
       "121992    every_DT move_NN mistake_NN panic_NN attack_NN...\n",
       "Name: tagged_tokens, Length: 162411, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebc6d5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162411"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get all words from the document with pos_tag\n",
    "\n",
    "tagged_tokens_list = []\n",
    "for index, value in train_X.iteritems():\n",
    "    tagged_tokens_list.append(value)\n",
    "len(tagged_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c58669b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164687"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combining the document and dictionary\n",
    "tagged_tokens_list.extend(list_of_lem)\n",
    "len(tagged_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ba26c",
   "metadata": {},
   "source": [
    "### TF-IDF vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0595cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "tfidf_vector = tf_idf.fit(tagged_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b5e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder = LabelEncoder()\n",
    "# train_y = Encoder.fit_transform(train_y)\n",
    "# test_y = Encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = tfidf_vector.transform(train_X)\n",
    "#print dimension of data\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming test data into tf-idf matrix\n",
    "X_test_tf = tfidf_vector.transform(test_X)\n",
    "\n",
    "#print dimension of data\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef4263",
   "metadata": {},
   "source": [
    "### Word2Vec Vectorisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b42080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [nltk.word_tokenize(sentence.lower()) for sentence in tagged_tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826cff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3199c38",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b21f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_tf, train_y)\n",
    "#predicted y\n",
    "y_pred_nb = naive_bayes_classifier.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b058420",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y, y_pred_nb, target_names=['Non-Suicide', 'Suicide']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715ad751",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(y_pred_nb, test_y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Confusion Matrix\n",
    "print(\"Naive Bayes Confusion Matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_confusion_matrix = metrics.confusion_matrix(test_y, y_pred_nb)\n",
    "\n",
    "nb_cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = nb_confusion_matrix, display_labels = [\"Non-Suicide\", \"Suicide\"])\n",
    "\n",
    "nb_cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=test_y)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a8212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert test_y and y_pred_logreg to numpy arrays\n",
    "test_y = test_y.to_numpy()\n",
    "y_pred_nb = np.array(y_pred_nb)\n",
    "\n",
    "# find misclassified samples\n",
    "nb_misclassified_indices = [i for i in range(len(test_y)) if test_y[i] != y_pred_nb[i]]\n",
    "print(len(nb_misclassified_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with test_X and their true and predicted labels\n",
    "df_test_nb = pd.DataFrame({'text': test_X, 'true_label': test_y, 'predicted_label': y_pred_nb})\n",
    "\n",
    "nb_misclassified_df = df_test_nb.iloc[nb_misclassified_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a745f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_misclassified_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddcdf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge misclassified_df with data based on the index\n",
    "nb_misclassified_merged = pd.merge(nb_misclassified_df, data_text, left_index=True, right_index=True)\n",
    "\n",
    "# print the misclassified texts, true labels, and predicted labels\n",
    "nb_misclassified_merged_df = pd.DataFrame(nb_misclassified_merged[['text_y', 'true_label', 'predicted_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe9981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_misclassified_merged_df.to_csv(\"misclassified_naive_bayes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4055449d",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the Logistic Regression classifier\n",
    "logreg = LogisticRegression(max_iter=200)\n",
    "logreg.fit(X_train_tf,train_y)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_logreg = logreg.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79881593",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y, y_pred_logreg, target_names=['Non-Suicide', 'Suicide']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5992fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression Accuracy Score -> \",accuracy_score(y_pred_logreg, test_y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5bb9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Reg Confusion Matrix\n",
    "print(\"Logistic Regression Confusion Matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcadb43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_confusion_matrix = metrics.confusion_matrix(test_y, y_pred_logreg)\n",
    "\n",
    "logreg_cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = logreg_confusion_matrix, display_labels = [\"Non-Suicide\", \"Suicide\"])\n",
    "\n",
    "logreg_cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert test_y and y_pred_logreg to numpy arrays\n",
    "y_pred_logreg = np.array(y_pred_logreg)\n",
    "\n",
    "# find misclassified samples\n",
    "logreg_misclassified_indices = [i for i in range(len(test_y)) if test_y[i] != y_pred_logreg[i]]\n",
    "print(len(logreg_misclassified_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with test_X and their true and predicted labels\n",
    "df_test_logreg = pd.DataFrame({'text': test_X, 'true_label': test_y, 'predicted_label': y_pred_logreg})\n",
    "\n",
    "logreg_misclassified_df = df_test_logreg.iloc[logreg_misclassified_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_misclassified_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge misclassified_df with data based on the index\n",
    "logreg_misclassified_merged = pd.merge(logreg_misclassified_df, data_text, left_index=True, right_index=True)\n",
    "\n",
    "# print the misclassified texts, true labels, and predicted labels\n",
    "logreg_misclassified_merged_df = pd.DataFrame(logreg_misclassified_merged[['text_y', 'true_label', 'predicted_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6aee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_misclassified_merged_df.to_csv(\"misclassified_logistic_regression.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9602b4f",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c650a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the SVM classifier\n",
    "linearSVC = LinearSVC()\n",
    "linearSVC.fit(X_train_tf,train_y)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_SVC = linearSVC.predict(X_test_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5351ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_y, y_pred_SVC, target_names=['Non-Suicide', 'Suicide']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b20490",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM Accuracy Score -> \",accuracy_score(y_pred_SVC, test_y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM Confusion Matrix\n",
    "print(\"Logistic Regression Confusion Matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, y_pred_SVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf24635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_confusion_matrix = metrics.confusion_matrix(test_y, y_pred_SVC,)\n",
    "\n",
    "svc_cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = svc_confusion_matrix, display_labels = [\"Non-Suicide\", \"Suicide\"])\n",
    "\n",
    "logreg_cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be95e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert test_y and y_pred_logreg to numpy arrays\n",
    "y_pred_SVC = np.array(y_pred_SVC)\n",
    "\n",
    "# find misclassified samples\n",
    "svc_misclassified_indices = [i for i in range(len(test_y)) if test_y[i] != y_pred_SVC[i]]\n",
    "print(len(svc_misclassified_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd6078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with test_X and their true and predicted labels\n",
    "df_test_SVC = pd.DataFrame({'text': test_X, 'true_label': test_y, 'predicted_label': y_pred_SVC})\n",
    "\n",
    "SVC_misclassified_df = df_test_SVC.iloc[svc_misclassified_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f4e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_misclassified_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e04cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge misclassified_df with data based on the index\n",
    "SVC_misclassified_merged = pd.merge(SVC_misclassified_df, data_text, left_index=True, right_index=True)\n",
    "\n",
    "# print the misclassified texts, true labels, and predicted labels\n",
    "SVC_misclassified_merged_df = pd.DataFrame(SVC_misclassified_merged[['text_y', 'true_label', 'predicted_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a791eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_misclassified_merged_df.to_csv(\"misclassified_SVM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d13a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample prediction\n",
    "test = ['i want to die. cannot live any longer. help']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b70441",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = re.sub('[^a-zA-Z]', ' ', test[0])\n",
    "review = review.lower()\n",
    "review = review.split()\n",
    "stop_list = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "review = [lemmatizer.lemmatize(word) for word in review if not word in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6feebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to add POS tags in the form of word_POS\n",
    "def add_pos_tag(sentences):\n",
    "    tagged_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize sentence into words\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        # Get POS tags for words\n",
    "        pos_tags = nltk.pos_tag(words)\n",
    "        # Join word and POS tag into word_POS format\n",
    "        tagged_words = [f\"{word}_{pos}\" for word, pos in pos_tags]\n",
    "        # Join tagged words into sentence\n",
    "        tagged_sentence = \" \".join(tagged_words)\n",
    "        # Add tagged sentence to list\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "    return tagged_sentences\n",
    "\n",
    "test_tagged = add_pos_tag(review)\n",
    "print(test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed =[ ' '.join(test_tagged)]\n",
    "test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cea873",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = tf_idf.transform(test_processed)\n",
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231bb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "nb_result = naive_bayes_classifier.predict(test_input)[0]\n",
    "nb_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogReg\n",
    "logreg_result = logreg.predict(test_input)[0]\n",
    "logreg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da514a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "SVC_result = linearSVC.predict(test_input)[0]\n",
    "SVC_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f30e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
