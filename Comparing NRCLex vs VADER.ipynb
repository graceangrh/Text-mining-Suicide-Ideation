{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc80ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import ast\n",
    "from nrclex import NRCLex\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score, fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee51233e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>stemmed_processed_text</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ex wife threatening suicide recently i left my...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threaten', 'suicid', 'recent',...</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suicide', 'rece...</td>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'not', 'get', 'affect', 'compliment'...</td>\n",
       "      <td>['weird', 'not', 'get', 'affected', 'complimen...</td>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>finally is almost over so i can never hear has...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['final', 'almost', 'never', 'hear', 'bad', 'y...</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i need help just help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'help', 'help', 'cri', 'hard']</td>\n",
       "      <td>['need', 'help', 'help', 'cry', 'hard']</td>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i m so lost hello my name is adam and i ve bee...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggl', '...</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggling'...</td>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text        class  \\\n",
       "0           0  ex wife threatening suicide recently i left my...      suicide   \n",
       "1           1  am i weird i do not get affected by compliment...  non-suicide   \n",
       "2           2  finally is almost over so i can never hear has...  non-suicide   \n",
       "3           3       i need help just help me i am crying so hard      suicide   \n",
       "4           4  i m so lost hello my name is adam and i ve bee...      suicide   \n",
       "\n",
       "                              stemmed_processed_text  \\\n",
       "0  ['ex', 'wife', 'threaten', 'suicid', 'recent',...   \n",
       "1  ['weird', 'not', 'get', 'affect', 'compliment'...   \n",
       "2  ['final', 'almost', 'never', 'hear', 'bad', 'y...   \n",
       "3            ['need', 'help', 'help', 'cri', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggl', '...   \n",
       "\n",
       "                           lemmatized_processed_text  \\\n",
       "0  ['ex', 'wife', 'threatening', 'suicide', 'rece...   \n",
       "1  ['weird', 'not', 'get', 'affected', 'complimen...   \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3            ['need', 'help', 'help', 'cry', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggling'...   \n",
       "\n",
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation  \n",
       "0  0.078125      0.109375  \n",
       "1  0.133333      0.200000  \n",
       "2  0.100000      0.100000  \n",
       "3  0.000000      0.000000  \n",
       "4  0.042453      0.122642  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df = pd.read_csv(\"df_emotions.csv\")\n",
    "emotion_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f956ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation  \n",
       "0  0.078125      0.109375  \n",
       "1  0.133333      0.200000  \n",
       "2  0.100000      0.100000  \n",
       "3  0.000000      0.000000  \n",
       "4  0.042453      0.122642  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df.drop(columns = [\"text\",\"lemmatized_processed_text\",\"class\",\"stemmed_processed_text\",\"Unnamed: 0\"],axis=1,inplace=True)\n",
    "emotion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c6b468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>stemmed_processed_text</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ex wife threatening suicide recently i left my...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threaten', 'suicid', 'recent',...</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suicide', 'rece...</td>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.9655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'not', 'get', 'affect', 'compliment'...</td>\n",
       "      <td>['weird', 'not', 'get', 'affected', 'complimen...</td>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.0984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>finally is almost over so i can never hear has...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['final', 'almost', 'never', 'hear', 'bad', 'y...</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i need help just help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'help', 'help', 'cri', 'hard']</td>\n",
       "      <td>['need', 'help', 'help', 'cry', 'hard']</td>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i m so lost hello my name is adam and i ve bee...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggl', '...</td>\n",
       "      <td>['lost', 'hello', 'name', 'adam', 'struggling'...</td>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.9965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text        class  \\\n",
       "0           0  ex wife threatening suicide recently i left my...      suicide   \n",
       "1           1  am i weird i do not get affected by compliment...  non-suicide   \n",
       "2           2  finally is almost over so i can never hear has...  non-suicide   \n",
       "3           3       i need help just help me i am crying so hard      suicide   \n",
       "4           4  i m so lost hello my name is adam and i ve bee...      suicide   \n",
       "\n",
       "                              stemmed_processed_text  \\\n",
       "0  ['ex', 'wife', 'threaten', 'suicid', 'recent',...   \n",
       "1  ['weird', 'not', 'get', 'affect', 'compliment'...   \n",
       "2  ['final', 'almost', 'never', 'hear', 'bad', 'y...   \n",
       "3            ['need', 'help', 'help', 'cri', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggl', '...   \n",
       "\n",
       "                           lemmatized_processed_text  \\\n",
       "0  ['ex', 'wife', 'threatening', 'suicide', 'rece...   \n",
       "1  ['weird', 'not', 'get', 'affected', 'complimen...   \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3            ['need', 'help', 'help', 'cry', 'hard']   \n",
       "4  ['lost', 'hello', 'name', 'adam', 'struggling'...   \n",
       "\n",
       "                     clean_lemmatized_processed_text    neg    neu    pos  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.378  0.437  0.186   \n",
       "1  weird not get affected compliment coming someo...  0.225  0.529  0.245   \n",
       "2  finally almost never hear bad year ever swear ...  0.259  0.433  0.308   \n",
       "3                            need help help cry hard  0.413  0.092  0.495   \n",
       "4  lost hello name adam struggling year afraid pa...  0.368  0.505  0.127   \n",
       "\n",
       "   compound  \n",
       "0   -0.9655  \n",
       "1    0.0984  \n",
       "2    0.2025  \n",
       "3    0.2263  \n",
       "4   -0.9965  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df_2 = pd.read_csv(\"lexicon_approach2.csv\")\n",
    "emotion_df_2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "065a9563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>suicide</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.9655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.0984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.9965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class    neg    neu    pos  compound\n",
       "0      suicide  0.378  0.437  0.186   -0.9655\n",
       "1  non-suicide  0.225  0.529  0.245    0.0984\n",
       "2  non-suicide  0.259  0.433  0.308    0.2025\n",
       "3      suicide  0.413  0.092  0.495    0.2263\n",
       "4      suicide  0.368  0.505  0.127   -0.9965"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df_2.drop(columns = [\"text\",\"lemmatized_processed_text\",\"stemmed_processed_text\",\"Unnamed: 0\",\"clean_lemmatized_processed_text\"],axis=1,inplace=True)\n",
    "emotion_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd8fddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex_NN wife_NN threatening_VBG suicide_NN recen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weird_JJ not_RB get_VB affected_VBN compliment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally_RB almost_RB never_RB hear_VB bad_JJ y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need_NN help_NN help_NN cry_NN hard_RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lost_VBN hello_UH name_NN adam_NN struggling_V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tagged_tokens\n",
       "0  ex_NN wife_NN threatening_VBG suicide_NN recen...\n",
       "1  weird_JJ not_RB get_VB affected_VBN compliment...\n",
       "2  finally_RB almost_RB never_RB hear_VB bad_JJ y...\n",
       "3             need_NN help_NN help_NN cry_NN hard_RB\n",
       "4  lost_VBN hello_UH name_NN adam_NN struggling_V..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####get POS_tagged\n",
    "pos_df = pd.read_csv('pos_tagged_new_negation.csv')\n",
    "\n",
    "pos_df.dropna(inplace=True)\n",
    "\n",
    "pos_df['tagged_tokens'] = pos_df['tagged_tokens'].apply(lambda x: [str(i) for i in ast.literal_eval(x)])\n",
    "\n",
    "pos_df.drop(columns = [\"text\",\"lemmatized_processed_text\",\"tokens\",\"class\"], inplace = True)\n",
    "pos_df[\"tagged_tokens\"] = pos_df[\"tagged_tokens\"].apply(lambda x: \" \".join(x))\n",
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e07fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([emotion_df, emotion_df_2,pos_df ], axis=1)\n",
    "combined_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf9b6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_lemmatized_processed_text</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticip</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>class</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>tagged_tokens</th>\n",
       "      <th>norm_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suicide recently left wife...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.186</td>\n",
       "      <td>-0.9655</td>\n",
       "      <td>ex_NN wife_NN threatening_VBG suicide_NN recen...</td>\n",
       "      <td>0.01725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weird not get affected compliment coming someo...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.0984</td>\n",
       "      <td>weird_JJ not_RB get_VB affected_VBN compliment...</td>\n",
       "      <td>0.54920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally almost never hear bad year ever swear ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.2025</td>\n",
       "      <td>finally_RB almost_RB never_RB hear_VB bad_JJ y...</td>\n",
       "      <td>0.60125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>need help help cry hard</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>need_NN help_NN help_NN cry_NN hard_RB</td>\n",
       "      <td>0.61315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lost hello name adam struggling year afraid pa...</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.103774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.198113</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.061321</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>suicide</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.9965</td>\n",
       "      <td>lost_VBN hello_UH name_NN adam_NN struggling_V...</td>\n",
       "      <td>0.00175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     clean_lemmatized_processed_text      fear     anger  \\\n",
       "0  ex wife threatening suicide recently left wife...  0.125000  0.125000   \n",
       "1  weird not get affected compliment coming someo...  0.066667  0.000000   \n",
       "2  finally almost never hear bad year ever swear ...  0.100000  0.100000   \n",
       "3                            need help help cry hard  0.000000  0.000000   \n",
       "4  lost hello name adam struggling year afraid pa...  0.146226  0.103774   \n",
       "\n",
       "   anticip     trust  surprise  positive  negative   sadness   disgust  \\\n",
       "0      0.0  0.078125  0.078125  0.078125  0.171875  0.093750  0.062500   \n",
       "1      0.0  0.133333  0.133333  0.133333  0.133333  0.000000  0.066667   \n",
       "2      0.0  0.150000  0.050000  0.150000  0.100000  0.050000  0.100000   \n",
       "3      0.0  0.000000  0.000000  0.000000  0.500000  0.500000  0.000000   \n",
       "4      0.0  0.066038  0.037736  0.075472  0.198113  0.146226  0.061321   \n",
       "\n",
       "        joy  anticipation        class    neg    neu    pos  compound  \\\n",
       "0  0.078125      0.109375      suicide  0.378  0.437  0.186   -0.9655   \n",
       "1  0.133333      0.200000  non-suicide  0.225  0.529  0.245    0.0984   \n",
       "2  0.100000      0.100000  non-suicide  0.259  0.433  0.308    0.2025   \n",
       "3  0.000000      0.000000      suicide  0.413  0.092  0.495    0.2263   \n",
       "4  0.042453      0.122642      suicide  0.368  0.505  0.127   -0.9965   \n",
       "\n",
       "                                       tagged_tokens  norm_compound  \n",
       "0  ex_NN wife_NN threatening_VBG suicide_NN recen...        0.01725  \n",
       "1  weird_JJ not_RB get_VB affected_VBN compliment...        0.54920  \n",
       "2  finally_RB almost_RB never_RB hear_VB bad_JJ y...        0.60125  \n",
       "3             need_NN help_NN help_NN cry_NN hard_RB        0.61315  \n",
       "4  lost_VBN hello_UH name_NN adam_NN struggling_V...        0.00175  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Normalizing Data as naive bayes dont accept negative\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "combined_df['norm_compound'] = scaler.fit_transform(combined_df[['compound']])\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15aae198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_test, y_pred):\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(conf_matrix)\n",
    "    TP = conf_matrix[0][0]\n",
    "    FN = conf_matrix[1][0]\n",
    "    FP = conf_matrix[0][1]\n",
    "    TN = conf_matrix[1][1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    precision = precision_score(y_test, y_pred)*100\n",
    "    recall = recall_score(y_test, y_pred)*100\n",
    "\n",
    "    print('TP:',TP); print('FN:',FN); print('FP:',FP) ;print('TN:',TN)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "\n",
    "    f1_score = 2*((precision * recall) / ((precision + recall)))\n",
    "    print('F1 Score:', f1_score)\n",
    "\n",
    "    f2score = ((1 + 2**2) * precision * recall) / (2**2 * precision + recall)\n",
    "    print('F2 Score:', f2score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19062412",
   "metadata": {},
   "source": [
    "### TESTING 3 VECTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e28c50d",
   "metadata": {},
   "source": [
    "#### NRC FLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "995e83a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##reading the domain dict from the text file\n",
    "with open(\"domain_dict_postag_lem.txt\", \"r\") as file:\n",
    "    list_of_lem = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe1d5892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231979"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tokens_list = []\n",
    "for index, value in combined_df['tagged_tokens'].items():\n",
    "    tagged_tokens_list.append(value)\n",
    "len(tagged_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac25b564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231979"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tokens_list.extend(list_of_lem)\n",
    "len(tagged_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "953247dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "# Create a sparse matrix for the extra features\n",
    "extra_features_sparse = sp.csr_matrix(combined_df[[\"fear\", \"anger\", \"anticip\", \"trust\", \"surprise\", \"positive\", \"negative\", \"sadness\",\"disgust\", \"joy\", \"anticipation\"]].values)\n",
    "\n",
    "# Fit and transform the TF-IDF matrix\n",
    "tfidf_vectorizer_sa_nrc = TfidfVectorizer()\n",
    "tfidf_sa_nrc = tfidf_vectorizer_sa_nrc.fit_transform(tagged_tokens_list)\n",
    "\n",
    "# Combine the two sparse matrices using hstack\n",
    "all_features_sa_nrc = sp.hstack([tfidf_sa_nrc, extra_features_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0736c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_sa_nrc, test_X_sa_nrc, train_y_sa_nrc, test_y_sa_nrc = train_test_split(all_features_sa_nrc,combined_df['class'],test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d73df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 162385, n_features: 47588\n",
      "n_samples: 69594, n_features: 47588\n"
     ]
    }
   ],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % train_X_sa_nrc.shape)\n",
    "print(\"n_samples: %d, n_features: %d\" % test_X_sa_nrc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ea9c41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162385, 47588)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_sa_nrc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bf4b620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208365    non-suicide\n",
       "49348         suicide\n",
       "163403    non-suicide\n",
       "116631    non-suicide\n",
       "149784    non-suicide\n",
       "             ...     \n",
       "139580        suicide\n",
       "128493    non-suicide\n",
       "185478    non-suicide\n",
       "214175    non-suicide\n",
       "212553    non-suicide\n",
       "Name: class, Length: 69594, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sa_nrc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76953aa3",
   "metadata": {},
   "source": [
    "#### VADER1  (Pos, neg, neu, comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45fd94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features_sparse = sp.csr_matrix(combined_df[[\"pos\",\"neg\",\"neu\",\"norm_compound\"]].values)\n",
    "\n",
    "# Fit and transform the TF-IDF matrix\n",
    "tfidf_vectorizer_sa_va_1 = TfidfVectorizer()\n",
    "tfidf_sa_va_1 = tfidf_vectorizer_sa_va_1.fit_transform(tagged_tokens_list)\n",
    "\n",
    "# Combine the two sparse matrices using hstack\n",
    "all_features_sa_va_1 = sp.hstack([tfidf_sa_va_1, extra_features_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a1e6e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_sa_va_1, test_X_sa_va_1, train_y_sa_va_1, test_y_sa_va_1 = train_test_split(all_features_sa_va_1,combined_df['class'],test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d35c102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 162385, n_features: 47581\n",
      "n_samples: 69594, n_features: 47581\n"
     ]
    }
   ],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % train_X_sa_va_1.shape)\n",
    "print(\"n_samples: %d, n_features: %d\" % test_X_sa_va_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e89df",
   "metadata": {},
   "source": [
    "#### VADER2  (comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e15aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_features_sparse = sp.csr_matrix(combined_df[[\"norm_compound\"]].values)\n",
    "\n",
    "# Fit and transform the TF-IDF matrix\n",
    "tfidf_vectorizer_sa_va_2 = TfidfVectorizer()\n",
    "tfidf_sa_va_2 = tfidf_vectorizer_sa_va_2.fit_transform(tagged_tokens_list)\n",
    "\n",
    "# Combine the two sparse matrices using hstack\n",
    "all_features_sa_va_2 = sp.hstack([tfidf_sa_va_2, extra_features_sparse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "327a2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_sa_va_2, test_X_sa_va_2, train_y_sa_va_2, test_y_sa_va_2 = train_test_split(all_features_sa_va_2,combined_df['class'],test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "077ed783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 162385, n_features: 47578\n",
      "n_samples: 69594, n_features: 47578\n"
     ]
    }
   ],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % train_X_sa_va_2.shape)\n",
    "print(\"n_samples: %d, n_features: %d\" % test_X_sa_va_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5398ddfd",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc2d63",
   "metadata": {},
   "source": [
    "#### NRC LEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "844cc482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "naive_bayes_classifier_sa_nrc = MultinomialNB()\n",
    "naive_bayes_classifier_sa_nrc.fit(train_X_sa_nrc, train_y_sa_nrc)\n",
    "#predicted y\n",
    "y_pred_nb_sa_nrc = naive_bayes_classifier_sa_nrc.predict(test_X_sa_nrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc191d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  89.66002816334742\n"
     ]
    }
   ],
   "source": [
    "nb_accuracy_sa_nrc = accuracy_score(y_pred_nb_sa_nrc, test_y_sa_nrc)\n",
    "print(\"Naive Bayes Accuracy Score -> \", nb_accuracy_sa_nrc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a6b8913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-Suicide     0.9449    0.8425    0.8908     34833\n",
      "     Suicide     0.8577    0.9508    0.9018     34761\n",
      "\n",
      "    accuracy                         0.8966     69594\n",
      "   macro avg     0.9013    0.8967    0.8963     69594\n",
      "weighted avg     0.9013    0.8966    0.8963     69594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y_sa_nrc, y_pred_nb_sa_nrc, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b3808ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29348  5485]\n",
      " [ 1711 33050]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(test_y_sa_nrc, y_pred_nb_sa_nrc)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc349012",
   "metadata": {},
   "source": [
    "#### VADER1  (Pos, neg, neu, comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a63a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "naive_bayes_classifier_sa_va_1 = MultinomialNB()\n",
    "naive_bayes_classifier_sa_va_1.fit(train_X_sa_va_1, train_y_sa_va_1)\n",
    "#predicted y\n",
    "y_pred_nb_sa_va_1 = naive_bayes_classifier_sa_va_1.predict(test_X_sa_va_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb612ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  90.4316464062994\n"
     ]
    }
   ],
   "source": [
    "nb_accuracy_sa_va_1 = accuracy_score(y_pred_nb_sa_va_1, test_y_sa_va_1)\n",
    "print(\"Naive Bayes Accuracy Score -> \", nb_accuracy_sa_va_1 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4bd122a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score: 0.9043568413229315\n"
     ]
    }
   ],
   "source": [
    "recall = recall_score(test_y_sa_va_1, y_pred_nb_sa_va_1, average='macro')\n",
    "print(\"Recall score:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d380216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-Suicide     0.9387    0.8653    0.9005     34833\n",
      "     Suicide     0.8749    0.9434    0.9078     34761\n",
      "\n",
      "    accuracy                         0.9043     69594\n",
      "   macro avg     0.9068    0.9044    0.9042     69594\n",
      "weighted avg     0.9068    0.9043    0.9042     69594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y_sa_va_1, y_pred_nb_sa_va_1, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2da5e9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30142  4691]\n",
      " [ 1968 32793]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(test_y_sa_va_1, y_pred_nb_sa_va_1)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc953540",
   "metadata": {},
   "source": [
    "#### VADER1  (comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dac7b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "naive_bayes_classifier_sa_va_2 = MultinomialNB()\n",
    "naive_bayes_classifier_sa_va_2.fit(train_X_sa_va_2, train_y_sa_va_2)\n",
    "#predicted y\n",
    "y_pred_nb_sa_va_2 = naive_bayes_classifier_sa_va_2.predict(test_X_sa_va_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e5c5331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  90.05086645400465\n"
     ]
    }
   ],
   "source": [
    "nb_accuracy_sa_va_2 = accuracy_score(y_pred_nb_sa_va_2, test_y_sa_va_2)\n",
    "print(\"Naive Bayes Accuracy Score -> \", nb_accuracy_sa_va_2 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d303b8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-Suicide     0.9416    0.8542    0.8958     34833\n",
      "     Suicide     0.8663    0.9470    0.9048     34761\n",
      "\n",
      "    accuracy                         0.9005     69594\n",
      "   macro avg     0.9040    0.9006    0.9003     69594\n",
      "weighted avg     0.9040    0.9005    0.9003     69594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y_sa_va_2, y_pred_nb_sa_va_2, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4323271a",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8faab",
   "metadata": {},
   "source": [
    "#### NRC LEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd4acb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the Logistic Regression classifier\n",
    "logreg_sa_nrc = LogisticRegression(max_iter=200)\n",
    "logreg_sa_nrc.fit(train_X_sa_nrc,train_y_sa_nrc)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_logreg_sa_nrc = logreg_sa_nrc.predict(test_X_sa_nrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b60d2e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score ->  93.56553725895911\n"
     ]
    }
   ],
   "source": [
    "logreg_accuracy_sa_nrc = accuracy_score(y_pred_logreg_sa_nrc, test_y_sa_nrc)\n",
    "print(\"Logistic Regression Accuracy Score -> \", logreg_accuracy_sa_nrc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "363171ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-Suicide     0.9276    0.9452    0.9363     34833\n",
      "     Suicide     0.9440    0.9261    0.9350     34761\n",
      "\n",
      "    accuracy                         0.9357     69594\n",
      "   macro avg     0.9358    0.9356    0.9356     69594\n",
      "weighted avg     0.9358    0.9357    0.9356     69594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y_sa_nrc, y_pred_logreg_sa_nrc, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc7dd5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32924  1909]\n",
      " [ 2569 32192]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(test_y_sa_nrc, y_pred_logreg_sa_nrc)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6c4ca",
   "metadata": {},
   "source": [
    "#### VADER1  (Pos, neg, neu, comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d045071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the Logistic Regression classifier\n",
    "logreg_sa_va_1 = LogisticRegression(max_iter=1000)\n",
    "logreg_sa_va_1.fit(train_X_sa_va_1,train_y_sa_va_1)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_logreg_sa_va_1 = logreg_sa_va_1.predict(test_X_sa_va_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c83eaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score ->  93.60002299048769\n"
     ]
    }
   ],
   "source": [
    "logreg_accuracy_sa_va_1 = accuracy_score(y_pred_logreg_sa_va_1, test_y_sa_va_1)\n",
    "print(\"Logistic Regression Accuracy Score -> \", logreg_accuracy_sa_va_1 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69aaa658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-Suicide     0.9281    0.9454    0.9367     34833\n",
      "     Suicide     0.9443    0.9266    0.9353     34761\n",
      "\n",
      "    accuracy                         0.9360     69594\n",
      "   macro avg     0.9362    0.9360    0.9360     69594\n",
      "weighted avg     0.9362    0.9360    0.9360     69594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y_sa_va_1, y_pred_logreg_sa_va_1, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ab37226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32932  1901]\n",
      " [ 2553 32208]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(test_y_sa_va_1, y_pred_logreg_sa_va_1)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cf536",
   "metadata": {},
   "source": [
    "#### VADER1  (comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36c86493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the Logistic Regression classifier\n",
    "logreg_sa_va_2 = LogisticRegression(max_iter=200)\n",
    "logreg_sa_va_2.fit(train_X_sa_va_2,train_y_sa_va_2)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_logreg_sa_va_2 = logreg_sa_va_2.predict(test_X_sa_va_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aef47c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score ->  93.57128488088054\n"
     ]
    }
   ],
   "source": [
    "logreg_accuracy_sa_va_2 = accuracy_score(y_pred_logreg_sa_va_2, test_y_sa_va_2)\n",
    "print(\"Logistic Regression Accuracy Score -> \", logreg_accuracy_sa_va_2 * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d5d648e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-Suicide     0.9270    0.9461    0.9364     34833\n",
      "     Suicide     0.9449    0.9253    0.9350     34761\n",
      "\n",
      "    accuracy                         0.9357     69594\n",
      "   macro avg     0.9359    0.9357    0.9357     69594\n",
      "weighted avg     0.9359    0.9357    0.9357     69594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y_sa_va_2, y_pred_logreg_sa_va_2, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb44f3",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768bfdcb",
   "metadata": {},
   "source": [
    "#### NRC FLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d5607a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the SVM classifier\n",
    "linearSVC_sa_nrc = LinearSVC()\n",
    "linearSVC_sa_nrc.fit(train_X_sa_nrc,train_y_sa_nrc)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_SVC_sa_nrc = linearSVC_sa_nrc.predict(test_X_sa_nrc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2eb1a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  93.5080610397448\n"
     ]
    }
   ],
   "source": [
    "svc_accuracy_sa_nrc = accuracy_score(y_pred_SVC_sa_nrc, test_y_sa_nrc)\n",
    "print(\"SVM Accuracy Score -> \",svc_accuracy_sa_nrc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1cebeacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-Suicide     0.9286    0.9428    0.9356     34833\n",
      "     Suicide     0.9418    0.9273    0.9345     34761\n",
      "\n",
      "    accuracy                         0.9351     69594\n",
      "   macro avg     0.9352    0.9351    0.9351     69594\n",
      "weighted avg     0.9352    0.9351    0.9351     69594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y_sa_nrc, y_pred_SVC_sa_nrc, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62e5a13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32842  1991]\n",
      " [ 2527 32234]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(test_y_sa_nrc, y_pred_SVC_sa_nrc)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc221aef",
   "metadata": {},
   "source": [
    "#### VADER1  (Pos, neg, neu, comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f7dc502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the SVM classifier\n",
    "linearSVC_sa_va_1 = LinearSVC()\n",
    "linearSVC_sa_va_1.fit(train_X_sa_va_1,train_y_sa_va_1)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_SVC_sa_va_1 = linearSVC_sa_va_1.predict(test_X_sa_va_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a070e4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  93.53823605483231\n"
     ]
    }
   ],
   "source": [
    "svc_accuracy_sa_va_1 = accuracy_score(y_pred_SVC_sa_va_1, test_y_sa_va_1)\n",
    "print(\"SVM Accuracy Score -> \",svc_accuracy_sa_va_1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c42a765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-Suicide     0.9293    0.9426    0.9359     34833\n",
      "     Suicide     0.9417    0.9281    0.9348     34761\n",
      "\n",
      "    accuracy                         0.9354     69594\n",
      "   macro avg     0.9355    0.9354    0.9354     69594\n",
      "weighted avg     0.9355    0.9354    0.9354     69594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y_sa_va_1, y_pred_SVC_sa_va_1, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7fd31a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32835  1998]\n",
      " [ 2499 32262]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(test_y_sa_va_1, y_pred_SVC_sa_va_1)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3c1eb",
   "metadata": {},
   "source": [
    "#### VADER1  (comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f26505c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the SVM classifier\n",
    "linearSVC_sa_va_2 = LinearSVC()\n",
    "linearSVC_sa_va_2.fit(train_X_sa_va_2,train_y_sa_va_2)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_SVC_sa_va_2 = linearSVC_sa_va_2.predict(test_X_sa_va_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fb7335f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  93.53679914935196\n"
     ]
    }
   ],
   "source": [
    "svc_accuracy_sa_va_2 = accuracy_score(y_pred_SVC_sa_va_2, test_y_sa_va_2)\n",
    "print(\"SVM Accuracy Score -> \",svc_accuracy_sa_va_2*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62305c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Non-Suicide     0.9289    0.9431    0.9359     34833\n",
      "     Suicide     0.9421    0.9276    0.9348     34761\n",
      "\n",
      "    accuracy                         0.9354     69594\n",
      "   macro avg     0.9355    0.9354    0.9354     69594\n",
      "weighted avg     0.9355    0.9354    0.9354     69594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y_sa_va_2, y_pred_SVC_sa_va_2, target_names=['Non-Suicide', 'Suicide'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908442a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
