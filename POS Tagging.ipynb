{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffd68441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f871928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>stemmed_processed_text</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suiciderecently i left my ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threaten', 'suiciderec', 'left...</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suiciderecently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'get', 'affect', 'compliment', 'come...</td>\n",
       "      <td>['weird', 'get', 'affected', 'compliment', 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally      is almost over    so i can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['final', 'almost', 'never', 'hear', 'bad', 'y...</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need helpjust help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'helpjust', 'help', 'cri', 'hard']</td>\n",
       "      <td>['need', 'helpjust', 'help', 'cry', 'hard']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m so losthello  my name is adam      and i v...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['losthello', 'name', 'adam', 'struggl', 'year...</td>\n",
       "      <td>['losthello', 'name', 'adam', 'struggling', 'y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class  \\\n",
       "0  ex wife threatening suiciderecently i left my ...      suicide   \n",
       "1  am i weird i do not get affected by compliment...  non-suicide   \n",
       "2  finally      is almost over    so i can never ...  non-suicide   \n",
       "3        i need helpjust help me i am crying so hard      suicide   \n",
       "4  i m so losthello  my name is adam      and i v...      suicide   \n",
       "\n",
       "                              stemmed_processed_text  \\\n",
       "0  ['ex', 'wife', 'threaten', 'suiciderec', 'left...   \n",
       "1  ['weird', 'get', 'affect', 'compliment', 'come...   \n",
       "2  ['final', 'almost', 'never', 'hear', 'bad', 'y...   \n",
       "3        ['need', 'helpjust', 'help', 'cri', 'hard']   \n",
       "4  ['losthello', 'name', 'adam', 'struggl', 'year...   \n",
       "\n",
       "                           lemmatized_processed_text  \n",
       "0  ['ex', 'wife', 'threatening', 'suiciderecently...  \n",
       "1  ['weird', 'get', 'affected', 'compliment', 'co...  \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...  \n",
       "3        ['need', 'helpjust', 'help', 'cry', 'hard']  \n",
       "4  ['losthello', 'name', 'adam', 'struggling', 'y...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the csv data\n",
    "data = pd.read_csv(\"preprocessed_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e37bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop stemmed_processed_text column\n",
    "data.drop(columns = [\"stemmed_processed_text\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dee9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean and tokenize lemmatized_processed_text as it is intepreted as an entire string \n",
    "def clean_and_tokenize(text):\n",
    "    # Remove the brackets and commas using a regular expression\n",
    "    cleaned_text = re.sub(r\"[\\[\\],']\", \"\", text)\n",
    "    # Tokenize the string using word_tokenize\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa52a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'lemmatized_processed_text' column of the DataFrame\n",
    "data['tokens'] = data['lemmatized_processed_text'].apply(clean_and_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "494e64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add POS tags to a list of tokens\n",
    "def add_pos_tags(tokens):\n",
    "    tagged_tokens = []\n",
    "    for token in tokens:\n",
    "        pos_tag = nltk.pos_tag([token])[0][1]\n",
    "        tagged_token = f\"{token}_{pos_tag}\"\n",
    "        tagged_tokens.append(tagged_token)\n",
    "    return tagged_tokens\n",
    "\n",
    "# Apply the function to the 'tokens' column of the DataFrame\n",
    "data['tagged_tokens'] = data['tokens'].map(add_pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a89ab5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>lemmatized_processed_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ex wife threatening suiciderecently i left my ...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['ex', 'wife', 'threatening', 'suiciderecently...</td>\n",
       "      <td>[ex, wife, threatening, suiciderecently, left,...</td>\n",
       "      <td>[ex_NN, wife_NN, threatening_VBG, suiciderecen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am i weird i do not get affected by compliment...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['weird', 'get', 'affected', 'compliment', 'co...</td>\n",
       "      <td>[weird, get, affected, compliment, coming, som...</td>\n",
       "      <td>[weird_NN, get_VB, affected_JJ, compliment_NN,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>finally      is almost over    so i can never ...</td>\n",
       "      <td>non-suicide</td>\n",
       "      <td>['finally', 'almost', 'never', 'hear', 'bad', ...</td>\n",
       "      <td>[finally, almost, never, hear, bad, year, ever...</td>\n",
       "      <td>[finally_RB, almost_RB, never_RB, hear_NN, bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i need helpjust help me i am crying so hard</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['need', 'helpjust', 'help', 'cry', 'hard']</td>\n",
       "      <td>[need, helpjust, help, cry, hard]</td>\n",
       "      <td>[need_NN, helpjust_NN, help_NN, cry_NN, hard_JJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m so losthello  my name is adam      and i v...</td>\n",
       "      <td>suicide</td>\n",
       "      <td>['losthello', 'name', 'adam', 'struggling', 'y...</td>\n",
       "      <td>[losthello, name, adam, struggling, year, afra...</td>\n",
       "      <td>[losthello_NN, name_NN, adam_NN, struggling_VB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        class  \\\n",
       "0  ex wife threatening suiciderecently i left my ...      suicide   \n",
       "1  am i weird i do not get affected by compliment...  non-suicide   \n",
       "2  finally      is almost over    so i can never ...  non-suicide   \n",
       "3        i need helpjust help me i am crying so hard      suicide   \n",
       "4  i m so losthello  my name is adam      and i v...      suicide   \n",
       "\n",
       "                           lemmatized_processed_text  \\\n",
       "0  ['ex', 'wife', 'threatening', 'suiciderecently...   \n",
       "1  ['weird', 'get', 'affected', 'compliment', 'co...   \n",
       "2  ['finally', 'almost', 'never', 'hear', 'bad', ...   \n",
       "3        ['need', 'helpjust', 'help', 'cry', 'hard']   \n",
       "4  ['losthello', 'name', 'adam', 'struggling', 'y...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [ex, wife, threatening, suiciderecently, left,...   \n",
       "1  [weird, get, affected, compliment, coming, som...   \n",
       "2  [finally, almost, never, hear, bad, year, ever...   \n",
       "3                  [need, helpjust, help, cry, hard]   \n",
       "4  [losthello, name, adam, struggling, year, afra...   \n",
       "\n",
       "                                       tagged_tokens  \n",
       "0  [ex_NN, wife_NN, threatening_VBG, suiciderecen...  \n",
       "1  [weird_NN, get_VB, affected_JJ, compliment_NN,...  \n",
       "2  [finally_RB, almost_RB, never_RB, hear_NN, bad...  \n",
       "3   [need_NN, helpjust_NN, help_NN, cry_NN, hard_JJ]  \n",
       "4  [losthello_NN, name_NN, adam_NN, struggling_VB...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
