{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da70d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf818874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "data = pd.read_csv(\"pos_tagged_new.csv\")\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03d8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "data['tagged_tokens'] = data['tagged_tokens'].apply(lambda x: [str(i) for i in ast.literal_eval(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb5e7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>suicide</td>\n",
       "      <td>[ex_NN, wife_NN, threatening_VBG, suicide_NN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>[weird_JJ, get_VB, affected_VBN, compliment_NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>[finally_RB, almost_RB, never_RB, hear_VB, bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide</td>\n",
       "      <td>[need_NN, help_NN, help_NN, cry_NN, hard_RB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide</td>\n",
       "      <td>[lost_VBN, hello_UH, name_NN, adam_NN, struggl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class                                      tagged_tokens\n",
       "0      suicide  [ex_NN, wife_NN, threatening_VBG, suicide_NN, ...\n",
       "1  non-suicide  [weird_JJ, get_VB, affected_VBN, compliment_NN...\n",
       "2  non-suicide  [finally_RB, almost_RB, never_RB, hear_VB, bad...\n",
       "3      suicide       [need_NN, help_NN, help_NN, cry_NN, hard_RB]\n",
       "4      suicide  [lost_VBN, hello_UH, name_NN, adam_NN, struggl..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(columns = [\"text\",\"lemmatized_processed_text\",\"tokens\"], inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a203dae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tagged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>suicide</td>\n",
       "      <td>ex_NN wife_NN threatening_VBG suicide_NN recen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>weird_JJ get_VB affected_VBN compliment_NN com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non-suicide</td>\n",
       "      <td>finally_RB almost_RB never_RB hear_VB bad_JJ y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suicide</td>\n",
       "      <td>need_NN help_NN help_NN cry_NN hard_RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suicide</td>\n",
       "      <td>lost_VBN hello_UH name_NN adam_NN struggling_V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class                                      tagged_tokens\n",
       "0      suicide  ex_NN wife_NN threatening_VBG suicide_NN recen...\n",
       "1  non-suicide  weird_JJ get_VB affected_VBN compliment_NN com...\n",
       "2  non-suicide  finally_RB almost_RB never_RB hear_VB bad_JJ y...\n",
       "3      suicide             need_NN help_NN help_NN cry_NN hard_RB\n",
       "4      suicide  lost_VBN hello_UH name_NN adam_NN struggling_V..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert to string of words\n",
    "data[\"tagged_tokens\"] = data[\"tagged_tokens\"].apply(lambda x: \" \".join(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5290727",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(data['tagged_tokens'],data['class'],test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15145304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf idf\n",
    "tf_idf = TfidfVectorizer()\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.fit_transform(train_X)\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf6b0551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 162411, n_features: 44044\n"
     ]
    }
   ],
   "source": [
    "#print dimension of data\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b9d1505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 69606, n_features: 44044\n"
     ]
    }
   ],
   "source": [
    "#transforming test data into tf-idf matrix\n",
    "X_test_tf = tf_idf.transform(test_X)\n",
    "\n",
    "#print dimension of data\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61bd81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_tf, train_y)\n",
    "#predicted y\n",
    "y_pred_nb = naive_bayes_classifier.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99030760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Suicide       0.95      0.83      0.88     34757\n",
      " Non-Suicide       0.85      0.95      0.90     34849\n",
      "\n",
      "    accuracy                           0.89     69606\n",
      "   macro avg       0.90      0.89      0.89     69606\n",
      "weighted avg       0.90      0.89      0.89     69606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y, y_pred_nb, target_names=['Suicide', 'Non-Suicide']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87023551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  89.1388673390225\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(y_pred_nb, test_y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b643181e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Confusion Matrix:\n",
      "[[28797  5960]\n",
      " [ 1600 33249]]\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Confusion Matrix\n",
    "print(\"Naive Bayes Confusion Matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17309367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training dataset on the Logistic Regression classifier\n",
    "logreg = LogisticRegression(max_iter=200)\n",
    "logreg.fit(X_train_tf,train_y)\n",
    "# predict the labels on validation dataset\n",
    "y_pred_logreg = logreg.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10cd2428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Suicide       0.93      0.95      0.94     34757\n",
      " Non-Suicide       0.95      0.93      0.94     34849\n",
      "\n",
      "    accuracy                           0.94     69606\n",
      "   macro avg       0.94      0.94      0.94     69606\n",
      "weighted avg       0.94      0.94      0.94     69606\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_y, y_pred_logreg, target_names=['Suicide', 'Non-Suicide']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aaf0170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score ->  93.73329885354711\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Accuracy Score -> \",accuracy_score(y_pred_logreg, test_y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76562fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Confusion Matrix:\n",
      "[[32920  1837]\n",
      " [ 2525 32324]]\n"
     ]
    }
   ],
   "source": [
    "#Log Reg Confusion Matrix\n",
    "print(\"Logistic Regression Confusion Matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "336bcfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample prediction\n",
    "test = ['i want to jump off a building. i dont want to live anymore. i want to die']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7844023",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = re.sub('[^a-zA-Z]', ' ', test[0])\n",
    "review = review.lower()\n",
    "review = review.split()\n",
    "stop_list = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "review = [lemmatizer.lemmatize(word) for word in review if not word in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f38fde09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['want_NN', 'jump_NN', 'building_NN', 'dont_NN', 'want_NN', 'live_JJ', 'anymore_RB', 'want_NN', 'die_NN']\n"
     ]
    }
   ],
   "source": [
    "# Create a function to add POS tags in the form of word_POS\n",
    "def add_pos_tag(sentences):\n",
    "    tagged_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Tokenize sentence into words\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        # Get POS tags for words\n",
    "        pos_tags = nltk.pos_tag(words)\n",
    "        # Join word and POS tag into word_POS format\n",
    "        tagged_words = [f\"{word}_{pos}\" for word, pos in pos_tags]\n",
    "        # Join tagged words into sentence\n",
    "        tagged_sentence = \" \".join(tagged_words)\n",
    "        # Add tagged sentence to list\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "    return tagged_sentences\n",
    "\n",
    "test_tagged = add_pos_tag(review)\n",
    "print(test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1cc5307c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['want_NN jump_NN building_NN dont_NN want_NN live_JJ anymore_RB want_NN die_NN']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_processed =[ ' '.join(test_tagged)]\n",
    "test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d967a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 44044)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = tf_idf.transform(test_processed)\n",
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9407d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'suicide'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "nb_result = naive_bayes_classifier.predict(test_input)[0]\n",
    "nb_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1c6f47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'suicide'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LogReg\n",
    "logreg_result = logreg.predict(test_input)[0]\n",
    "logreg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b798c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f84a2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
